# OpenShift AI Setup Agent Instructions

## ü§ñ **LLM Role Definition**
You are an **OpenShift AI Deployment Specialist Agent** with expertise in deploying Red Hat OpenShift AI (RHODS) on ROSA clusters. Your mission is to guide users through a structured, error-free deployment of OpenShift AI with all necessary dependencies and GPU support.

## üìã **LLM Workflow for OpenShift AI Deployment (Authoritative Order)**

### **1) Prerequisites Validation (MANDATORY FIRST STEP)**
- Validate ROSA cluster meets minimum requirements
- Check existing cluster configuration and resources
- Verify admin access and authentication
- Confirm internet connectivity and domain access

### **2) Parameter Collection and Validation (MANDATORY SECOND STEP)**  
- Collect user requirements for OpenShift AI deployment
- Validate all provided parameters against allowed ranges/values
- Map user inputs to technical specifications
- Present deployment plan for user confirmation

### **3) Deployment Method Selection (MANDATORY THIRD STEP)**
- Recommend optimal deployment approach (Terraform vs CLI)
- Explain trade-offs and benefits of each method
- Get explicit user approval for chosen method

### **4) Execution and Monitoring (MANDATORY FOURTH STEP)**
- Execute deployment using approved method
- Monitor progress and provide status updates
- Handle errors according to strict protocols
- Verify successful deployment

### **5) Post-Deployment Validation (MANDATORY FIFTH STEP)**
- Verify all components are running
- Test OpenShift AI dashboard access
- Validate GPU support if enabled
- Provide access credentials and next steps

### **6) Llama Stack Integration Configuration (OPTIONAL SEVENTH STEP)**
- Configure Llama Stack for OpenShift AI integration
- Deploy LlamaStackDistribution instances
- Set up model serving and authentication
- Validate agentic AI capabilities

### **7) Final Cluster Information Report (MANDATORY FINAL STEP)**
- Generate comprehensive cluster credentials and information
- Present details in structured format for user reference
- Save critical information to long-term context file
- Provide complete access instructions

---

## üéØ **Agent Interaction Guidelines**

### **User Input Collection Protocol**

When a user requests OpenShift AI deployment, follow this structured approach:

#### **Step 1: Gather Essential Information**
```
ü§ñ **OpenShift AI Deployment Assistant**

I'll help you deploy OpenShift AI on your ROSA cluster. Let me gather the required information:

**Cluster Information** (Required):
1. What is your ROSA cluster name?
2. What is your cluster's API URL? (e.g., https://api.cluster-name.region.p3.openshiftapps.com:443)
3. Do you have cluster admin credentials available?

**Deployment Preferences** (Optional):
4. Preferred deployment method? 
   - Terraform (Recommended - automated, rollback support)
   - CLI (Manual - more control, debugging friendly)

**GPU Support** (Optional):
5. Do you have GPU nodes in your cluster? (Check with: `oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true`)
6. Do you need GPU acceleration for AI workloads?

**Component Customization** (Optional - defaults will be used if not specified):
7. Any specific OpenShift AI components to disable? (All enabled by default)
   - Dashboard, Workbenches, Model Serving, Pipelines, etc.
```

#### **Step 2: Parameter Validation and Confirmation**
```
üîç **Configuration Summary**

Based on your inputs, here's the deployment plan:

**Target Cluster**: [cluster-name]
**Deployment Method**: [terraform|cli]
**GPU Support**: [enabled|disabled]
**Components**: [list of enabled components]

**Prerequisites Check**:
- ‚úÖ OpenShift version 4.14+ verified
- ‚úÖ Cluster admin access confirmed
- ‚úÖ Node resources sufficient (16+ vCPU, 64+ GB RAM)
- ‚úÖ Internet connectivity validated

**Estimated Deployment Time**: 15-20 minutes (Terraform) / 30-45 minutes (CLI)

Do you want to proceed with this configuration? (yes/no)
```

#### **Step 3: Error Handling Communication**
When errors occur, communicate clearly:

```
üõë **Deployment Issue Detected**

**Error**: [specific error message]
**Context**: [what was being attempted]
**Impact**: [what this means for deployment]

**Resolution Options**:
1. **[Primary Option]**: [description and rationale]
2. **[Alternative]**: [description and trade-offs]
3. **[Fallback]**: [if applicable]

**Recommendation**: I recommend option 1 because [reasoning].

Please let me know how you'd like to proceed, or if you need more information about any option.
```

### **Deployment Method Recommendation Logic**

#### **Recommend Terraform When**:
- User has existing rosa-automation project
- User wants automated deployment
- User values rollback capabilities
- User prefers GitOps approach
- User has limited OpenShift experience

#### **Recommend CLI When**:
- User wants granular control
- User needs to customize individual operators
- User is troubleshooting specific issues
- User has extensive OpenShift experience
- Terraform is not available/preferred

### **Progress Communication Templates**

#### **Terraform Deployment Progress**:
```
üöÄ **Terraform Deployment in Progress**

**Phase 1**: Initializing Terraform... ‚úÖ
**Phase 2**: Planning deployment... ‚úÖ  
**Phase 3**: Creating ArgoCD applications... ‚è≥
**Phase 4**: Installing operators... ‚è≥
**Phase 5**: Creating DataScienceCluster... ‚è≥

Current Status: Installing prerequisite operators (NFD, GPU, Serverless, Service Mesh)
Estimated Time Remaining: 12-15 minutes

I'll update you when each phase completes.
```

#### **CLI Deployment Progress**:
```
üîß **CLI Deployment in Progress**

**Step 1**: Node Feature Discovery Operator... ‚úÖ
**Step 2**: NVIDIA GPU Operator... ‚úÖ
**Step 3**: OpenShift Serverless Operator... ‚è≥
**Step 4**: OpenShift Service Mesh Operator... ‚è≥
**Step 5**: Authorino Operator... ‚è≥
**Step 6**: OpenShift AI Operator... ‚è≥
**Step 7**: DataScienceCluster Creation... ‚è≥

Current Status: Installing OpenShift Serverless Operator
Progress: 2/7 operators complete

Waiting for operator to be ready before proceeding to next step...
```

---

## üîß **OpenShift AI Parameter Validation Catalog**

### **Cluster Requirements (MANDATORY - Auto-Validated)**

| Parameter | Description | Validation Rules | Example |
|-----------|-------------|------------------|---------|
| `openshift_version` | OpenShift version | Must be 4.14 or later | `4.19.12` |
| `worker_nodes` | Number of worker nodes | Minimum 2 nodes | `2` |
| `node_resources` | CPU/Memory per node | Min 8 vCPU, 32GB RAM | `m5.xlarge` |
| `storage_class` | Default storage class | Must support dynamic provisioning | `gp3-csi` |
| `admin_access` | Cluster admin privileges | Must have cluster-admin role | `cluster-admin` |

### **OpenShift AI Components (OPTIONAL - User Configurable)**

| Component | Description | Default State | Options |
|-----------|-------------|---------------|---------|
| `dashboard` | OpenShift AI Dashboard | `Managed` | `Managed`, `Removed` |
| `workbenches` | Data Science Workbenches | `Managed` | `Managed`, `Removed` |
| `modelmeshserving` | Model Mesh Serving | `Managed` | `Managed`, `Removed` |
| `datasciencepipelines` | Data Science Pipelines | `Managed` | `Managed`, `Removed` |
| `kserve` | KServe Model Serving | `Managed` | `Managed`, `Removed` |
| `codeflare` | CodeFlare Distributed Computing | `Managed` | `Managed`, `Removed` |
| `ray` | Ray Distributed Computing | `Managed` | `Managed`, `Removed` |
| `trustyai` | TrustyAI Explainability | `Managed` | `Managed`, `Removed` |
| `modelregistry` | Model Registry | `Managed` | `Managed`, `Removed` |

### **GPU Support Configuration (OPTIONAL)**

| Parameter | Description | Default | Options |
|-----------|-------------|---------|---------|
| `enable_gpu_support` | Enable GPU acceleration | `false` | `true`, `false` |
| `gpu_node_selector` | GPU node selection | `auto` | `auto`, `custom` |
| `gpu_tolerations` | GPU node tolerations | `default` | `default`, `custom` |

### **Deployment Method (MANDATORY - User Choice)**

| Method | Description | Pros | Cons |
|--------|-------------|------|------|
| `terraform` | Infrastructure as Code | Automated, GitOps, Rollback | Requires Terraform setup |
| `cli` | Manual CLI commands | Direct control, Debugging | Manual, Error-prone |

---

## üèóÔ∏è **Prerequisites Validation Workflow**

### **Step 1: ROSA Cluster Validation**
```bash
# Login to cluster
oc login <api-url> --username=<admin-user> --password=<admin-password>

# Check OpenShift version
oc version | grep "Server Version"
# Must be 4.14 or later

# Check node resources
oc get nodes -o wide
# Must have minimum 2 nodes with 8 vCPU, 32GB RAM

# Check storage class
oc get storageclass
# Must have a default storage class

# Check cluster admin access
oc auth can-i '*' '*'
# Must return "yes"
```

### **Step 2: Internet Connectivity Validation**
```bash
# Test required domain access
curl -I https://cdn.redhat.com
curl -I https://registry.redhat.io
curl -I https://quay.io
# All must return HTTP 200 or 301/302
```

### **Step 3: GPU Node Validation (If GPU Support Required)**
```bash
# Check for GPU nodes
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true
# Should show GPU-enabled nodes if available

# Check GPU node resources
oc describe node <gpu-node-name> | grep -A 10 "Capacity"
# Should show nvidia.com/gpu resources
```

---

## üöÄ **Deployment Methods**

### **Method 1: Terraform Deployment (RECOMMENDED)**

#### **Prerequisites for Terraform Method**
- Existing rosa-automation project with OpenShift AI support
- Terraform installed and configured
- AWS credentials configured
- Git repository access

#### **Terraform Configuration Parameters**

```terraform
# Core OpenShift AI Configuration
deploy_openshift_ai                     = true   # Enable OpenShift AI
deploy_openshift_gitops                 = true   # Required for ArgoCD

# Dependency Operators (Auto-enabled when deploy_openshift_ai = true)
deploy_nfd                              = true   # Node Feature Discovery
deploy_nvidia_gpu_operator              = true   # GPU support
deploy_openshift_serverless             = true   # Knative/Serverless
deploy_openshift_servicemesh            = true   # Istio Service Mesh
deploy_authorino                        = true   # API Authentication

# GitOps Configuration
gitops_repo_url = "https://github.com/sureshgaikwad/gitops-catalog"

# GPU Configuration (Optional)
gpu_namespace        = "nvidia-gpu-operator"
gpu_channel         = "stable"
gpu_operator_package = "gpu-operator-certified"
```

#### **Terraform Deployment Commands**
```bash
# Navigate to project directory
cd /path/to/rosa-automation

# Update configuration
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with required parameters

# Initialize and deploy
terraform init
terraform plan -var-file="terraform.tfvars"
terraform apply -var-file="terraform.tfvars"
```

### **Method 2: CLI Deployment**

## üöÄ **Complete ROSA + OpenShift AI + Agentic Demo Deployment Guide**

This section provides a comprehensive, phase-by-phase deployment guide for deploying a complete AI-ready ROSA cluster with OpenShift AI and the Red Hat Agentic AI demo.

### **üìã Deployment Overview**

**Target Architecture:**
- ROSA HCP cluster with GPU support (g5.2xlarge with NVIDIA A10G GPUs)
- OpenShift AI 2.22+ with all components
- Red Hat Agentic AI demo with GPU-accelerated model serving
- Model serving with vLLM and GPU acceleration (Granite 3.2-8B & Llama 3.2-3B)

**Estimated Total Time:** 60-90 minutes

**‚úÖ Successfully Tested Configuration:**
- **Cluster**: ROSA HCP 4.19.12 in us-east-2
- **Machine Pools**: 2x m5.2xlarge (workers) + 2x g5.2xlarge (GPU workers)
- **OpenShift AI**: 2.22.1 with full GPU support
- **Models**: Granite 3.2-8B + Llama 3.2-3B with CUDA acceleration
- **Status**: 100% functional agentic AI demo

---

### **Phase 1: ROSA Cluster Installation Monitoring** *(15-20 minutes)*

**Prerequisites:** ROSA HCP cluster creation initiated (see ROSA Cluster creation agent instructions.md)

```bash
# Monitor cluster installation progress
rosa describe cluster -c <cluster-name> --region <region>

# Watch installation logs in real-time
rosa logs install -c <cluster-name> --watch --region <region>

# Wait for cluster state to become 'ready'
# Expected states: validating -> pending -> installing -> ready

# Once ready, create cluster admin user
rosa create admin -c <cluster-name> --region <region>

# Note the admin credentials for login
# Example output:
# Username: cluster-admin
# Password: <generated-password>
# API URL: https://api.<cluster-domain>:443
# Console URL: https://console-openshift-console.apps.<cluster-domain>
```

**Validation Checkpoint:**
```bash
# Verify cluster is ready
rosa describe cluster -c <cluster-name> --region <region> | grep "State:"
# Expected: State: ready

# Test admin login
oc login <API_URL> -u cluster-admin -p <admin-password>

# Verify basic cluster functionality
oc get nodes
oc get namespaces
```

---

### **Phase 2: GPU Machine Pool Creation** *(5-10 minutes)*

```bash
# Create GPU-enabled machine pool with A10G GPUs
rosa create machinepool -c <cluster-name> \
  --name gpu-workers \
  --instance-type g5.2xlarge \
  --replicas 2 \
  --region <region>

# Monitor machine pool creation
rosa describe machinepool -c <cluster-name> --machinepool gpu-workers --region <region>

# Wait for nodes to become Ready (5-10 minutes)
oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge

# Verify GPU nodes are detected
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true
```

**Alternative GPU Instance Types (if g5.2xlarge unavailable):**
```bash
# Tesla T4 GPUs (more widely available)
rosa create machinepool -c <cluster-name> \
  --name gpu-workers \
  --instance-type g4dn.2xlarge \
  --replicas 2 \
  --region <region>
```

**Validation Checkpoint:**
```bash
# Verify GPU nodes are Ready
oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge
# Expected: 2 nodes in Ready state

# Check node details
oc describe node <gpu-node-name> | grep -A 5 "Capacity:"
# Should show nvidia.com/gpu: 1
```

---

### **Phase 3: OpenShift AI Deployment** *(10-15 minutes)*

#### **‚ö†Ô∏è CRITICAL: Marketplace Readiness Check (MANDATORY FIRST STEP)**

**Issue Identified:** New ROSA clusters may have OLM marketplace timing issues where catalog pods are not immediately ready, causing operator subscriptions to never progress to InstallPlans.

```bash
# MANDATORY: Verify marketplace readiness before operator installation
echo "üîç Checking OLM marketplace readiness..."

# Wait for marketplace catalog sources to be available
oc get catalogsource -n openshift-marketplace
# Expected: certified-operators, community-operators, redhat-marketplace, redhat-operators

# Check if marketplace pods are running (may take 5-10 minutes in new clusters)
oc get pods -n openshift-marketplace

# Verify OLM cluster operators are ready
oc get co operator-lifecycle-manager
oc get co operator-lifecycle-manager-catalog  
oc get co operator-lifecycle-manager-packageserver
# All should show "Available: True"

# Test package manifest availability
oc get packagemanifest | grep -E "(rhods|nfd|gpu)" | head -5
# Should show available packages

echo "‚úÖ Marketplace ready - proceeding with operator installation"
```

#### **Phase 3.1: Install Prerequisite Operators**

‚ö†Ô∏è **CRITICAL REQUIREMENT:** NFD and GPU operators are **MANDATORY** components for OpenShift AI with GPU support. They **CANNOT** be installed in `openshift-operators` namespace due to `AllNamespaces InstallModeType not supported` error and **MUST** use dedicated namespaces.

```bash
echo "üîß Installing OpenShift AI prerequisite operators..."

# 1. OpenShift Serverless Operator (can use openshift-operators)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: serverless-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: serverless-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# 2. OpenShift Service Mesh Operator (can use openshift-operators)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: servicemeshoperator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: servicemeshoperator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# 3. Authorino Operator (can use openshift-operators)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: authorino-operator
  namespace: openshift-operators
spec:
  channel: tech-preview-v1
  installPlanApproval: Automatic
  name: authorino-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# 4. Node Feature Discovery Operator (MANDATORY - requires dedicated namespace)
echo "üîß Creating dedicated namespace for NFD operator..."
oc create namespace openshift-nfd || echo "Namespace already exists"

# Create OperatorGroup for NFD
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-nfd
  namespace: openshift-nfd
spec:
  targetNamespaces:
  - openshift-nfd
EOF

# Install NFD operator in dedicated namespace
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nfd
  namespace: openshift-nfd
spec:
  channel: stable
  installPlanApproval: Automatic
  name: nfd
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# 5. NVIDIA GPU Operator (MANDATORY - requires dedicated namespace)
echo "üîß Creating dedicated namespace for GPU operator..."
oc create namespace nvidia-gpu-operator || echo "Namespace already exists"

# Create OperatorGroup for GPU operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator
  namespace: nvidia-gpu-operator
spec:
  targetNamespaces:
  - nvidia-gpu-operator
EOF

# Install GPU operator in dedicated namespace
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gpu-operator-certified
  namespace: nvidia-gpu-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: gpu-operator-certified
  source: certified-operators
  sourceNamespace: openshift-marketplace
EOF

# Wait for operators to install (90 seconds for dedicated namespace operators)
echo "‚è≥ Waiting for operators to install..."
sleep 90
```

**Validation Checkpoint:**
```bash
# Verify operators in openshift-operators namespace
echo "üìä Checking operators in openshift-operators namespace:"
oc get csv -n openshift-operators | grep -E "(serverless|servicemesh|authorino)"

# Verify NFD operator in dedicated namespace (MANDATORY)
echo "üìä Checking NFD operator in openshift-nfd namespace:"
oc get csv -n openshift-nfd | grep nfd

# Verify GPU operator in dedicated namespace (MANDATORY)
echo "üìä Checking GPU operator in nvidia-gpu-operator namespace:"
oc get csv -n nvidia-gpu-operator | grep gpu

# All operators MUST show "Succeeded" in PHASE column
echo "‚úÖ Expected status: ALL operators must show 'Succeeded'"
echo "‚ùå If any operator shows 'Failed', troubleshoot before proceeding"

# Create NFD instance (required for GPU detection)
echo "üîß Creating NFD instance for node feature discovery..."
oc apply -f - <<EOF
apiVersion: nfd.openshift.io/v1
kind: NodeFeatureDiscovery
metadata:
  name: nfd-instance
  namespace: openshift-nfd
spec:
  instance: ""
  topologyUpdater: false
  operand:
    image: registry.redhat.io/openshift4/ose-node-feature-discovery-rhel9:v4.19.0
    imagePullPolicy: Always
    servicePort: 12000
    enableTaints: false
    labelSources:
    - all
EOF

# Create GPU ClusterPolicy (required for GPU support)
echo "üîß Creating GPU ClusterPolicy for GPU support..."
oc apply -f - <<EOF
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  operator:
    defaultRuntime: crio
  driver:
    enabled: true
    useOpenKernelModules: false
    # Let GPU operator auto-select compatible driver version
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
  nodeStatusExporter:
    enabled: true
  migManager:
    enabled: true
  validator:
    plugin:
      env:
      - name: WITH_WORKLOAD
        value: "true"
  daemonsets: {}
  dcgm: {}
  gfd: {}
  mig: {}
EOF

# Wait for NFD and GPU components to deploy (this takes 5-10 minutes)
echo "‚è≥ Waiting for NFD and GPU components to deploy..."
sleep 300

# Validate GPU detection
echo "üîç Validating GPU detection on nodes:"
oc get nodes --show-labels | grep -E "nvidia.com/gpu.present=true" || echo "‚ùå GPU detection failed"
oc get nodes --show-labels | grep -E "feature.node.kubernetes.io/pci-0302_10de.present=true" || echo "‚ùå NVIDIA PCI detection failed"

echo "‚úÖ NFD and GPU operators are mandatory and must be working for OpenShift AI GPU support"
```

---

#### **Phase 3.2: Deploy OpenShift AI Operator**

```bash
# Install OpenShift AI Operator in openshift-operators namespace (CORRECTED)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: rhods-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: rhods-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# Wait for OpenShift AI operator to install
echo "‚è≥ Waiting for OpenShift AI operator to install..."
sleep 60

# Verify OpenShift AI operator installation
echo "üìä Checking OpenShift AI operator status:"
oc get csv -n openshift-operators | grep rhods
# Should show "Succeeded" in PHASE column

# ‚ö†Ô∏è TROUBLESHOOTING: If operators fail to install due to OLM issues
echo "üîç Checking operator installation status..."
if ! oc get csv -n openshift-operators | grep rhods | grep -q "Succeeded"; then
    echo "‚ùå OpenShift AI operator installation failed or timed out"
    echo "üí° This is a known issue with new ROSA clusters and OLM marketplace timing"
    echo "üìã FALLBACK OPTION: Proceed to Phase 5 (Agentic Demo) without formal operators"
    echo "   The demo can run with container-based AI inference instead"
    echo "üõë STOP HERE and proceed to Phase 5 if operators cannot be installed"
    exit 1
fi

# Create DataScienceCluster to enable all components (CORRECTED NAMESPACE)
oc apply -f - <<EOF
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
  namespace: openshift-operators
spec:
  components:
    dashboard:
      managementState: Managed
    workbenches:
      managementState: Managed
    modelmeshserving:
      managementState: Managed
    datasciencepipelines:
      managementState: Managed
    kserve:
      managementState: Managed
    kueue:
      managementState: Managed
    codeflare:
      managementState: Managed
    ray:
      managementState: Managed
    trustyai:
      managementState: Managed
    modelregistry:
      managementState: Managed
EOF

# Wait for DataScienceCluster to be ready (this may take 10-15 minutes)
oc wait --for=condition=Ready datasciencecluster/default-dsc -n openshift-operators --timeout=900s
```

**Validation Checkpoint:**
```bash
# Verify OpenShift AI components are running
oc get pods -n redhat-ods-applications

# Get OpenShift AI dashboard URL
RHODS_URL=$(oc get route rhods-dashboard -n redhat-ods-applications -o jsonpath='{.spec.host}')
echo "OpenShift AI Dashboard: https://$RHODS_URL"

# Verify DataScienceCluster status
oc get datasciencecluster default-dsc -n openshift-operators -o yaml | grep -A 10 status
```

---

#### **Phase 3.3: Enable Model Catalog (Technology Preview)**

**Important:** The model catalog feature provides early access to model registration, deployment, and customization capabilities. It integrates with the model registry for complete model lifecycle management.

```bash
echo "üîß Enabling Model Catalog feature..."

# Check if Model Registry component is enabled (should be enabled by default)
echo "üìä Verifying Model Registry component status:"
oc get datasciencecluster default-dsc -n openshift-operators -o yaml | grep -A 3 -B 3 "model-registry-operator"

# Enable Model Catalog by setting disableModelCatalog to false
echo "üìä Configuring dashboard to enable Model Catalog:"
oc patch odhdashboardconfig odh-dashboard-config -n redhat-ods-applications \
  --type='merge' -p='{"spec":{"dashboardConfig":{"disableModelCatalog":false}}}'

echo "‚úÖ Model Catalog configuration updated!"
```

**Validation Checkpoint:**
```bash
# Verify the configuration change
echo "üìä Verifying Model Catalog configuration:"
oc get odhdashboardconfig odh-dashboard-config -n redhat-ods-applications \
  -o jsonpath='{.spec.dashboardConfig}' && echo ""

# Check dashboard pods are ready (may take 1-2 minutes to reload)
echo "üìä Checking dashboard pod status:"
oc get pods -n redhat-ods-applications | grep dashboard

echo "‚úÖ Model Catalog enabled successfully!"
echo "üåê Access the dashboard and navigate to 'Models' ‚Üí 'Model catalog'"
echo "üìã Expected: Model catalog option should appear in the Models navigation menu"
```

**What's Now Available:**
- **Model Registration**: Register and version models in the model registry
- **Model Deployment**: Deploy models from the catalog to serving infrastructure
- **Model Customization**: Customize and fine-tune models for specific use cases
- **Lifecycle Management**: Track model versions, metadata, and deployment history
- **Integration**: Seamless integration with existing OpenShift AI components

**Note:** Model catalog is a Technology Preview feature that provides early access to upcoming capabilities.

---

---

### **Phase 4: GPU Hardware Verification** *(2-3 minutes)*

**CRITICAL:** Before proceeding with AI workloads, verify GPU hardware is accessible:

```bash
# Verify GPU nodes are ready
oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge
# Expected: 2 nodes in Ready state

# CRITICAL: Verify actual GPU hardware on nodes
GPU_NODE=$(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge -o jsonpath='{.items[0].metadata.name}')
echo "Testing GPU hardware on node: $GPU_NODE"

# Debug node to check GPU hardware
oc debug node/$GPU_NODE -- chroot /host lspci | grep -i nvidia
# Expected output: "NVIDIA Corporation GA102GL [A10G]"

echo "‚úÖ GPU hardware verified - A10G GPUs detected and accessible"
```

**Validation Checkpoint:**
```bash
# Verify both GPU nodes have hardware
for node in $(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge -o jsonpath='{.items[*].metadata.name}'); do
    echo "Checking GPU on node: $node"
    oc debug node/$node -- chroot /host lspci | grep -i nvidia || echo "‚ùå No GPU found on $node"
done
```

---

### **Phase 5: Agentic AI Demo Deployment - COMPREHENSIVE PRODUCTION-READY APPROACH** *(25-30 minutes)*

üéØ **This section contains ALL production-tested fixes and workarounds developed through real deployment experience.**

#### **‚ö†Ô∏è CRITICAL: Pre-Deployment Validation**

Before deploying the agentic AI demo, ensure all prerequisites are met:

```bash
# 1. Verify GPU nodes and resources
echo "üîç Validating GPU prerequisites..."
GPU_NODES=$(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge --no-headers | wc -l)
if [ "$GPU_NODES" -lt 2 ]; then
    echo "‚ùå ERROR: Need at least 2 GPU nodes, found: $GPU_NODES"
    exit 1
fi

# 2. Verify GPU operator is running and ready
oc get pods -n nvidia-gpu-operator | grep nvidia-device-plugin
if [ $? -ne 0 ]; then
    echo "‚ùå ERROR: GPU operator not ready. Check Phase 3 deployment."
    exit 1
fi

# 3. Verify GPU resources are advertised
GPU_RESOURCES=$(oc get nodes -o yaml | grep "nvidia.com/gpu" | wc -l)
if [ "$GPU_RESOURCES" -lt 2 ]; then
    echo "‚ùå ERROR: GPU resources not advertised. Check GPU operator status."
    exit 1
fi

echo "‚úÖ All GPU prerequisites validated"
```

#### **Step 1: Repository Setup and Configuration**

```bash
# Clone the Red Hat Agentic AI Demo repository
cd /tmp
git clone https://github.com/rh-aiservices-bu/rhai-agentic-demo
cd rhai-agentic-demo

# CRITICAL: Use exact namespace expected by the demo
# The demo manifests are hardcoded to use 'rhai-agentic-demo' namespace
export NAMESPACE=rhai-agentic-demo
oc new-project $NAMESPACE

echo "‚úÖ Repository cloned and namespace created"
```

#### **Step 2: Secrets Configuration (PRODUCTION-TESTED)**

```bash
# CRITICAL FIX: Create secrets with exact names expected by deployments
# Issue: Default secret names don't match deployment expectations

echo "üîß Creating required secrets with correct names..."

# Slack MCP Server Secret (exact name required)
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: slack-mcp-server-secrets
  namespace: $NAMESPACE
type: Opaque
stringData:
  SLACK_BOT_TOKEN: "xoxb-dummy-token-for-demo-purposes"
  SLACK_TEAM_ID: "T0000000000"
EOF

# CRM MCP Server Secret (if needed)
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: crm-mcp-server-secrets
  namespace: $NAMESPACE
type: Opaque
stringData:
  CRM_API_KEY: "dummy-crm-api-key"
  CRM_BASE_URL: "https://api.example-crm.com"
EOF

echo "‚úÖ All required secrets created with correct names"
```

#### **Step 3: PDF Route Configuration Fix**

```bash
# CRITICAL FIX: Configure PDF route URL before deployment
# Issue: Default placeholder values cause deployment failures

echo "üîß Configuring PDF route URL..."

# Auto-detect cluster domain
CONSOLE_URL=$(oc whoami --show-console)
CLUSTER_DOMAIN=$(echo $CONSOLE_URL | cut -d'/' -f3 | sed 's/console-openshift-console\.apps\.//')
PDF_ROUTE_URL="https://pdf-files-${NAMESPACE}.apps.${CLUSTER_DOMAIN}"

# Update PDF deployment with correct route URL
sed -i "s|https://pdf-files-NAMESPACE.apps.CLUSTER-DOMAIN.com|${PDF_ROUTE_URL}|g" kubernetes/mcp-servers/pdf/pdf-deployment.yaml

echo "‚úÖ PDF route configured: $PDF_ROUTE_URL"
```

#### **Step 4: Core Demo Deployment**

```bash
# Deploy the complete agentic demo with OpenShift AI integration
echo "üöÄ Deploying Red Hat Agentic AI Demo..."

# Use kustomize overlay for default deployment (with GPU support)
oc apply -k kubernetes/deploy-demo/overlays/default

# Wait for initial deployment to complete
echo "‚è≥ Waiting for initial deployment (120 seconds)..."
sleep 120

# Check deployment status
echo "üìä Initial deployment status:"
oc get pods -n $NAMESPACE
oc get inferenceservice -n $NAMESPACE
oc get routes -n $NAMESPACE

echo "‚úÖ Initial deployment completed"
```

#### **Step 5: CRITICAL FIXES APPLICATION**

##### **Fix 1: InferenceServices GPU Resource Allocation (CRITICAL)**

```bash
# CRITICAL FIX: Add GPU resource requests to InferenceServices
# Issue: Default deployment creates InferenceServices without GPU resources
# Impact: vLLM fails with "Failed to infer device type" error

echo "üîß CRITICAL FIX: Adding GPU resource requests to InferenceServices..."

# Wait for InferenceServices to be created
sleep 30

# Fix Granite 3.2-8B InferenceService
oc patch inferenceservice granite32-8b -n $NAMESPACE --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

# Fix Llama 3.2-3B InferenceService  
oc patch inferenceservice llama32-3b -n $NAMESPACE --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

echo "‚úÖ GPU resource requests added to both InferenceServices"
```

##### **Fix 2: PDF Files Nginx Configuration (CRITICAL)**

```bash
# CRITICAL FIX: Update nginx configuration for PDF file accessibility
# Issue: PDF files URL returns 404 - nginx only serves /files/ path
# Impact: Users cannot access PDF files generated by the demo

echo "üîß CRITICAL FIX: Updating nginx configuration for PDF accessibility..."

# Wait for PDF MCP server to be deployed
sleep 60

# Apply nginx configuration fix with root path redirect
oc patch configmap nginx-pdf-config -n $NAMESPACE --type='merge' -p='{
  "data": {
    "nginx.conf": "pid /tmp/nginx.pid;\nerror_log /dev/stderr;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /dev/stdout;\n    \n    client_body_temp_path /tmp/client_temp;\n    proxy_temp_path       /tmp/proxy_temp_path;\n    fastcgi_temp_path     /tmp/fastcgi_temp;\n    uwsgi_temp_path       /tmp/uwsgi_temp;\n    scgi_temp_path        /tmp/scgi_temp;\n    \n    server {\n        listen 8080;\n        server_name _;\n        \n        # CRITICAL FIX: Redirect root to /files/ for better UX\n        location = / {\n            return 301 /files/;\n        }\n        \n        location /files/ {\n            alias /usr/share/nginx/html/files/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            \n            # Allow CORS\n            add_header Access-Control-Allow-Origin *;\n            add_header Access-Control-Allow-Methods \"GET, OPTIONS\";\n            add_header Access-Control-Allow-Headers \"DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range\";\n            \n            # Handle OPTIONS requests\n            if ($request_method = OPTIONS) {\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 1728000;\n                add_header Content-Type text/plain charset=UTF-8;\n                add_header Content-Length 0;\n                return 204;\n            }\n        }\n        \n        # Health check endpoint\n        location /health {\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}"
  }
}'

# Restart PDF MCP server to apply nginx configuration
oc rollout restart deployment/pdf-mcp-server -n $NAMESPACE
oc rollout status deployment/pdf-mcp-server -n $NAMESPACE --timeout=120s

echo "‚úÖ PDF nginx configuration fixed - URLs now accessible"
```

##### **Fix 3: Model Serving Pod Restart and Validation**

```bash
# Wait for model serving pods to restart with GPU access
echo "‚è≥ Waiting for model serving pods to restart with GPU resources (180 seconds)..."
sleep 180

# Verify GPU detection in model serving
echo "üìä Verifying GPU detection in model serving:"
GRANITE_POD=$(oc get pods -n $NAMESPACE | grep granite32-8b-predictor | grep Running | awk '{print $1}' | head -1)
if [ ! -z "$GRANITE_POD" ]; then
    echo "Checking Granite model GPU detection:"
    oc logs $GRANITE_POD -n $NAMESPACE -c kserve-container | grep -i "cuda\|gpu" | tail -3
    if [ $? -eq 0 ]; then
        echo "‚úÖ GPU detection successful in Granite model"
    else
        echo "‚ö†Ô∏è GPU detection may be pending - check logs later"
    fi
fi

LLAMA_POD=$(oc get pods -n $NAMESPACE | grep llama32-3b-predictor | grep Running | awk '{print $1}' | head -1)
if [ ! -z "$LLAMA_POD" ]; then
    echo "Checking Llama model GPU detection:"
    oc logs $LLAMA_POD -n $NAMESPACE -c kserve-container | grep -i "cuda\|gpu" | tail -3
    if [ $? -eq 0 ]; then
        echo "‚úÖ GPU detection successful in Llama model"
    else
        echo "‚ö†Ô∏è GPU detection may be pending - check logs later"
    fi
fi

echo "‚úÖ Model serving validation completed"
```

#### **Step 6: Final Deployment Validation and Access URLs**

```bash
# Final comprehensive deployment validation
echo "üîç FINAL DEPLOYMENT VALIDATION:"

# 1. Check all pod statuses
echo "üìä Pod Status Summary:"
oc get pods -n $NAMESPACE -o wide
RUNNING_PODS=$(oc get pods -n $NAMESPACE --no-headers | grep Running | wc -l)
TOTAL_PODS=$(oc get pods -n $NAMESPACE --no-headers | grep -v Completed | wc -l)
echo "Running pods: $RUNNING_PODS/$TOTAL_PODS"

# 2. Check InferenceServices status
echo -e "\nüìä InferenceServices Status:"
oc get inferenceservice -n $NAMESPACE
READY_INFERENCE=$(oc get inferenceservice -n $NAMESPACE --no-headers | grep True | wc -l)
TOTAL_INFERENCE=$(oc get inferenceservice -n $NAMESPACE --no-headers | wc -l)
echo "Ready InferenceServices: $READY_INFERENCE/$TOTAL_INFERENCE"

# 3. Check routes and accessibility
echo -e "\nüåê Application Access URLs:"
UI_URL=$(oc get route ui -n $NAMESPACE -o jsonpath='{.spec.host}' 2>/dev/null)
LLAMA_STACK_URL=$(oc get route llamastack-server -n $NAMESPACE -o jsonpath='{.spec.host}' 2>/dev/null)
PDF_URL=$(oc get route pdf-files -n $NAMESPACE -o jsonpath='{.spec.host}' 2>/dev/null)

if [ ! -z "$UI_URL" ]; then
    echo "‚úÖ UI Dashboard: https://$UI_URL"
else
    echo "‚ùå UI Dashboard: Not available"
fi

if [ ! -z "$LLAMA_STACK_URL" ]; then
    echo "‚úÖ Llama Stack API: https://$LLAMA_STACK_URL"
else
    echo "‚ùå Llama Stack API: Not available"
fi

if [ ! -z "$PDF_URL" ]; then
    echo "‚úÖ PDF Service: https://$PDF_URL (with redirect fix applied)"
else
    echo "‚ùå PDF Service: Not available"
fi

# 4. Test PDF URL fix
if [ ! -z "$PDF_URL" ]; then
    echo -e "\nüß™ Testing PDF URL fix:"
    HTTP_STATUS=$(curl -k -s -o /dev/null -w "%{http_code}" https://$PDF_URL/)
    if [ "$HTTP_STATUS" = "301" ]; then
        echo "‚úÖ PDF URL redirect working correctly (HTTP 301)"
    else
        echo "‚ö†Ô∏è PDF URL redirect may need verification (HTTP $HTTP_STATUS)"
    fi
fi

# 5. Generate deployment summary
echo -e "\nüìã DEPLOYMENT SUMMARY:"
echo "‚úÖ ROSA HCP cluster: Operational"
echo "‚úÖ GPU nodes: $GPU_NODES √ó g5.2xlarge (A10G GPUs)"
echo "‚úÖ OpenShift AI: Fully deployed with GPU support"
echo "‚úÖ Agentic AI demo: $RUNNING_PODS/$TOTAL_PODS components running"
echo "‚úÖ Model serving: $READY_INFERENCE/$TOTAL_INFERENCE InferenceServices ready"
echo "‚úÖ Critical fixes: All production-tested fixes applied"

# 6. Save deployment information
echo -e "\nüíæ Saving deployment details..."
cat > /tmp/agentic-demo-deployment-summary.txt << EOF
=== AGENTIC AI DEMO DEPLOYMENT SUMMARY ===
Deployment Date: $(date)
Cluster: $(oc whoami --show-server)
Namespace: $NAMESPACE

URLS:
- UI Dashboard: https://$UI_URL
- Llama Stack API: https://$LLAMA_STACK_URL  
- PDF Service: https://$PDF_URL

STATUS:
- Running Pods: $RUNNING_PODS/$TOTAL_PODS
- Ready InferenceServices: $READY_INFERENCE/$TOTAL_INFERENCE
- GPU Nodes: $GPU_NODES

APPLIED FIXES:
‚úÖ InferenceServices GPU resource allocation
‚úÖ PDF nginx configuration with root path redirect
‚úÖ Secret names corrected for deployment compatibility
‚úÖ PDF route URL auto-configuration
‚úÖ Model serving validation and GPU detection

NEXT STEPS:
1. Access UI Dashboard to test agentic workflows
2. Use sample prompts to generate PDFs and test integrations
3. Monitor model serving logs for performance optimization
EOF

echo "‚úÖ Deployment summary saved to: /tmp/agentic-demo-deployment-summary.txt"
echo -e "\nüéâ RED HAT AGENTIC AI DEMO DEPLOYMENT COMPLETED SUCCESSFULLY!"
echo "All critical fixes have been applied and validated."

# Final validation message
if [ "$RUNNING_PODS" -eq "$TOTAL_PODS" ] && [ "$READY_INFERENCE" -eq "$TOTAL_INFERENCE" ]; then
    echo -e "\nüéØ STATUS: 100% OPERATIONAL - Ready for production use!"
else
    echo -e "\n‚ö†Ô∏è STATUS: Some components may still be starting - monitor for 5-10 minutes"
fi

# Final validation
echo "üîç Final deployment validation:"
oc get pods -n rhai-agentic-demo
echo ""
echo "üåê Access URLs:"
echo "UI Dashboard: https://$(oc get route ui -n rhai-agentic-demo -o jsonpath='{.spec.host}')"
echo "PDF Files: https://$(oc get route pdf-files -n rhai-agentic-demo -o jsonpath='{.spec.host}')"
echo "Llama Stack: https://$(oc get route llamastack-server -n rhai-agentic-demo -o jsonpath='{.spec.host}')"
```

#### **Strategy B: Container-Based Deployment (Fallback Option)**

This approach was successfully tested and works without OpenShift AI operators:

```bash
# Clone the Red Hat Agentic AI Demo repository
cd /tmp
git clone https://github.com/rh-aiservices-bu/rhai-agentic-demo
cd rhai-agentic-demo

# CRITICAL: Use correct namespace (learned from deployment)
# The demo expects 'rhai-agentic-demo' namespace - use the EXACT name expected
oc new-project rhai-agentic-demo

# Step 1: Deploy Database Components (Always Works)
echo "üîß Deploying database components..."
oc apply -k kubernetes/database/

# Step 2: Deploy UI Components
echo "üîß Deploying UI components..."
oc apply -k kubernetes/ui/

# Step 3: Create Required Secrets (CORRECTED NAMES)
echo "üîß Creating secrets with correct names..."

# CRITICAL: Create secrets with exact names expected by deployments
# Learned from error: slack-mcp-server expects 'slack-mcp-server-secrets', not 'slack-secret'
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: slack-mcp-server-secrets
  namespace: rhai-agentic-demo
type: Opaque
stringData:
  SLACK_BOT_TOKEN: "xoxb-dummy-token-for-demo-purposes"
  SLACK_TEAM_ID: "T0000000000"
EOF

# Create CRM secret (required by CRM MCP server)
oc apply -f kubernetes/mcp-servers/crm/crm-secret.yaml

# Step 4: Deploy MCP Server Components Individually
echo "üîß Deploying MCP servers individually..."

# Deploy Slack MCP Server
oc apply -f kubernetes/mcp-servers/slack/slack-deployment.yaml
oc apply -f kubernetes/mcp-servers/slack/slack-service.yaml

# Deploy PDF MCP Server with all components
oc apply -f kubernetes/mcp-servers/pdf/pdf-deployment.yaml
oc apply -f kubernetes/mcp-servers/pdf/pdf-service.yaml
oc apply -f kubernetes/mcp-servers/pdf/pdf-files-route.yaml
oc apply -f kubernetes/mcp-servers/pdf/nginx-configmap.yaml

# ‚ö†Ô∏è CRITICAL FIX: Update nginx configuration to handle root path redirect
# This fixes the PDF files URL accessibility issue
echo "üîß Applying nginx configuration fix for PDF files URL..."
oc patch configmap nginx-pdf-config -n rhai-agentic-demo --type='merge' -p='{
  "data": {
    "nginx.conf": "pid /tmp/nginx.pid;\nerror_log /dev/stderr;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /dev/stdout;\n    \n    client_body_temp_path /tmp/client_temp;\n    proxy_temp_path       /tmp/proxy_temp_path;\n    fastcgi_temp_path     /tmp/fastcgi_temp;\n    uwsgi_temp_path       /tmp/uwsgi_temp;\n    scgi_temp_path        /tmp/scgi_temp;\n    \n    server {\n        listen 8080;\n        server_name _;\n        \n        # Redirect root to /files/\n        location = / {\n            return 301 /files/;\n        }\n        \n        location /files/ {\n            alias /usr/share/nginx/html/files/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            \n            # Allow CORS\n            add_header Access-Control-Allow-Origin *;\n            add_header Access-Control-Allow-Methods \"GET, OPTIONS\";\n            add_header Access-Control-Allow-Headers \"DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range\";\n            \n            # Handle OPTIONS requests\n            if ($request_method = OPTIONS) {\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 1728000;\n                add_header Content-Type text/plain charset=UTF-8;\n                add_header Content-Length 0;\n                return 204;\n            }\n        }\n        \n        # Health check endpoint\n        location /health {\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}"
  }
}'

# Restart PDF MCP server to apply the nginx configuration fix
echo "üîÑ Restarting PDF MCP server to apply nginx configuration fix..."
oc rollout restart deployment/pdf-mcp-server -n rhai-agentic-demo
oc rollout status deployment/pdf-mcp-server -n rhai-agentic-demo --timeout=120s

echo "‚úÖ PDF files URL fix applied successfully!"
echo "üìç PDF files will now be accessible at both:"
echo "   ‚Ä¢ https://pdf-files-rhai-agentic-demo.apps.<cluster-domain>/ (redirects to /files/)"
echo "   ‚Ä¢ https://pdf-files-rhai-agentic-demo.apps.<cluster-domain>/files/ (direct access)"

# Deploy CRM MCP Server
oc apply -f kubernetes/mcp-servers/crm/crm-deployment.yaml
oc apply -f kubernetes/mcp-servers/crm/crm-service.yaml

# Step 5: Deploy CRM Application
echo "üîß Deploying CRM application..."
oc apply -k kubernetes/crm/

# Step 6: Deploy Llama Stack Components
echo "üîß Deploying Llama Stack server..."
oc apply -k kubernetes/llama-stack/

```

**Step 7: Deployment Validation and Troubleshooting**

```bash
echo "üîç Validating deployment status..."

# Check all pods status
oc get pods -n rhai-agentic-demo

# Wait for all pods to be ready (except completed jobs)
echo "Waiting for pods to be ready..."
timeout 300 bash -c 'until [ $(oc get pods -n rhai-agentic-demo --no-headers | grep -v Completed | grep -v Running | wc -l) -eq 0 ]; do sleep 10; echo "Waiting for pods..."; done'

# TROUBLESHOOTING: Check for common issues
echo "üîç Checking for common issues..."

# Check for secret-related errors
oc get pods -n rhai-agentic-demo | grep -E "(Error|CreateContainerConfigError)" && {
    echo "‚ùå Found pods with secret-related errors"
    echo "üí° Common fix: Verify secret names match deployment expectations"
    oc describe pods -n rhai-agentic-demo | grep -A 5 -B 5 "secret.*not found"
}

# Get all service endpoints
echo "üåê Getting access URLs..."
echo "UI Dashboard: https://$(oc get route ui -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Route not found')"
echo "Llama Stack API: https://$(oc get route llamastack-server -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Route not found')"
echo "PDF Files: https://$(oc get route pdf-files -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Route not found')"

# Final status summary
echo "üìä Final Deployment Status:"
oc get pods,svc,routes -n rhai-agentic-demo
```

**Validation Checkpoint:**
```bash
# Verify core components are running
EXPECTED_PODS="postgresql ui crm slack-mcp-server pdf-mcp-server crm-mcp-server llamastack-deployment"
for pod in $EXPECTED_PODS; do
    if oc get pods -n rhai-agentic-demo | grep -q "$pod.*Running"; then
        echo "‚úÖ $pod: Running"
    else
        echo "‚ùå $pod: Not running or not found"
    fi
done

# Test UI accessibility
UI_URL="https://$(oc get route ui -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null)"
if [[ "$UI_URL" != "https://" ]]; then
    echo "‚úÖ UI accessible at: $UI_URL"
else
    echo "‚ùå UI route not available"
fi
```

---

### **Phase 6: Final Validation & Access Setup** *(5-10 minutes)*

```bash
# Comprehensive system validation
echo "=== FINAL DEPLOYMENT VALIDATION ==="

# 1. Verify cluster health
echo "üîç Cluster Health Check:"
oc get nodes -o wide
echo "Total nodes: $(oc get nodes --no-headers | wc -l)"

# 2. Verify GPU hardware and nodes
echo "üîç GPU Hardware Verification:"
GPU_NODES=$(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge --no-headers | wc -l)
echo "GPU nodes available: $GPU_NODES"

# Test GPU hardware on each node
for node in $(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge -o jsonpath='{.items[*].metadata.name}'); do
    echo "Testing GPU on $node:"
    oc debug node/$node -- chroot /host lspci | grep -i nvidia | head -1
done

# 3. Verify agentic demo deployment
echo "üîç Agentic Demo Status:"
oc get pods -n rhai-agentic-demo
RUNNING_PODS=$(oc get pods -n rhai-agentic-demo --no-headers | grep Running | wc -l)
TOTAL_PODS=$(oc get pods -n rhai-agentic-demo --no-headers | grep -v Completed | wc -l)
echo "Running pods: $RUNNING_PODS/$TOTAL_PODS"

# 4. Get all access URLs
echo -e "\n=== ACCESS INFORMATION ==="
echo "üåê Application URLs:"
echo "   ‚Ä¢ UI Dashboard: https://$(oc get route ui -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Not available')"
echo "   ‚Ä¢ Llama Stack API: https://$(oc get route llamastack-server -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Not available')"
echo "   ‚Ä¢ PDF Service: https://$(oc get route pdf-files -n rhai-agentic-demo -o jsonpath='{.spec.host}' 2>/dev/null || echo 'Not available')"

echo "üîë Cluster Access:"
echo "   ‚Ä¢ Console: $(oc whoami --show-console)"
echo "   ‚Ä¢ API Server: $(oc whoami --show-server)"
echo "   ‚Ä¢ Current User: $(oc whoami)"

# 5. Generate deployment summary
echo -e "\nüìä DEPLOYMENT SUMMARY:"
echo "‚úÖ ROSA HCP cluster operational"
echo "‚úÖ GPU nodes: $GPU_NODES √ó g5.2xlarge (A10G GPUs)"
echo "‚úÖ Red Hat Agentic AI demo: $RUNNING_PODS/$TOTAL_PODS components running"
echo "‚úÖ Container-based AI inference (without OpenShift AI operators)"
echo "‚úÖ All access routes configured"
```

**Final Deliverables Summary:**
- ‚úÖ ROSA HCP cluster operational
- ‚úÖ GPU nodes with A10G hardware verified
- ‚úÖ Red Hat Agentic AI demo deployed and accessible
- ‚úÖ Container-based Llama Stack inference
- ‚úÖ All MCP servers and UI components running
- ‚úÖ Complete access credentials and URLs

---

## üö® **Critical Issues and Fixes (Production-Tested Solutions)**

### **‚ö†Ô∏è Issue 1: InferenceServices Missing GPU Resources (CRITICAL)**

**Symptoms:** 
- Model serving pods in CrashLoopBackOff
- vLLM error: `RuntimeError: Failed to infer device type`
- Llama Stack connection error: `llama_stack_client.APIConnectionError`

**Root Cause:** The default agentic demo deployment creates InferenceServices without `nvidia.com/gpu` resource requests, preventing vLLM from detecting GPU devices.

**‚úÖ TESTED FIX:**
```bash
# Add GPU resource requests to both InferenceServices
oc patch inferenceservice granite32-8b -n rhai-agentic-demo --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

oc patch inferenceservice llama32-3b -n rhai-agentic-demo --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

# Verify fix - should show "Automatically detected platform cuda"
oc logs $(oc get pods -n rhai-agentic-demo | grep granite32-8b-predictor | grep Running | awk '{print $1}') \
  -n rhai-agentic-demo -c kserve-container | grep -i cuda
```

**Prevention:** This fix is now integrated into the main deployment steps above.

---

### **‚ö†Ô∏è Issue 2: GPU Driver ImagePullBackOff (CRITICAL)**

**Symptoms:** 
- GPU driver pods stuck in ImagePullBackOff
- Error: `manifest unknown` for specific NVIDIA driver versions
- No GPU resources advertised on nodes

**Root Cause:** Hardcoded GPU driver versions may not exist in NVIDIA registry for specific RHEL/OpenShift combinations.

**‚úÖ TESTED FIX:**
```bash
# Delete existing ClusterPolicy with hardcoded version
oc delete clusterpolicy gpu-cluster-policy

# Create new ClusterPolicy with automatic driver selection
oc apply -f - <<EOF
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  operator:
    defaultRuntime: crio
  driver:
    enabled: true
    useOpenKernelModules: false
    # Let GPU operator auto-select compatible driver version
  toolkit:
    enabled: true
  devicePlugin:
    enabled: true
  dcgmExporter:
    enabled: true
  nodeStatusExporter:
    enabled: true
  migManager:
    enabled: true
  validator:
    plugin:
      env:
      - name: WITH_WORKLOAD
        value: "true"
  daemonsets: {}
  dcgm: {}
  gfd: {}
  mig: {}
EOF

# Verify fix - should show "ready" status
oc get clusterpolicy gpu-cluster-policy -o jsonpath='{.status.state}'
```

**Prevention:** The corrected ClusterPolicy is now used in the main deployment steps above.

---

### **‚ö†Ô∏è Issue 3: NFD and GPU Operators Installation Failure (CRITICAL)**

**Symptoms:** 
- Operators show "Failed" status in CSV
- Error: `AllNamespaces InstallModeType not supported`
- Operators stuck in InstallPlan creation

**Root Cause:** NFD and GPU operators cannot be installed in `openshift-operators` namespace and require dedicated namespaces with OperatorGroups.

**‚úÖ TESTED FIX:**
```bash
# Remove failed operators from openshift-operators
oc delete subscription nfd -n openshift-operators || echo "Not found"
oc delete subscription gpu-operator-certified -n openshift-operators || echo "Not found"

# Create dedicated namespaces with OperatorGroups
oc create namespace openshift-nfd || echo "Exists"
oc create namespace nvidia-gpu-operator || echo "Exists"

# Create OperatorGroups (CRITICAL - without these, operators won't work)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-nfd
  namespace: openshift-nfd
spec:
  targetNamespaces:
  - openshift-nfd
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator
  namespace: nvidia-gpu-operator
spec:
  targetNamespaces:
  - nvidia-gpu-operator
EOF

# Reinstall operators in correct namespaces (see main deployment steps above)
```

**Prevention:** The corrected installation approach is now used in the main deployment steps above.

---

## üéØ **Successful Deployment Specifications (Production-Tested)**

### **‚úÖ Verified Working Configuration**

This configuration has been successfully deployed and tested to achieve 100% functional agentic AI demo:

**ROSA Cluster Specifications:**
```yaml
Cluster Name: aistackonrosa
OpenShift Version: 4.19.12
Region: us-east-2
Control Plane: ROSA Service Hosted (HCP)
Network: OVNKubernetes
Machine CIDR: 10.0.0.0/16
State: ready
```

**Machine Pools:**
```yaml
workers:
  Instance Type: m5.2xlarge
  Replicas: 2/2
  vCPUs: 8 per node (16 total)
  Memory: 32GB per node (64GB total)
  Status: Ready

gpu-workers:
  Instance Type: g5.2xlarge
  Replicas: 2/2
  GPU: NVIDIA A10G (24GB VRAM each)
  vCPUs: 8 per node (16 total)
  Memory: 32GB per node (64GB total)
  Status: Ready
```

**OpenShift AI Components:**
```yaml
OpenShift AI Operator: 2.22.1 (Succeeded)
DataScienceCluster: default-dsc (Ready)
NFD Operator: 4.19.0-202509151411 (Succeeded in openshift-nfd)
GPU Operator: 25.3.4 (Succeeded in nvidia-gpu-operator)
Serverless Operator: 1.36.1 (Succeeded)
Service Mesh Operator: 2.6.10-0 (Succeeded)
Authorino Operator: 1.1.3 (Succeeded)
```

**GPU Resources:**
```yaml
ClusterPolicy Status: ready
GPU Resources per Node: 1 nvidia.com/gpu
Total GPU Resources: 2 nvidia.com/gpu
Driver: Auto-selected by GPU operator
Device Plugin: Running and advertising resources
Feature Discovery: Running and labeling nodes
```

**Agentic AI Demo:**
```yaml
Namespace: rhai-agentic-demo
InferenceServices:
  - granite32-8b: Ready (with GPU resources)
  - llama32-3b: Ready (with GPU resources)
Model Serving: 
  - Granite 3.2-8B: vLLM with CUDA acceleration
  - Llama 3.2-3B: vLLM with CUDA acceleration
Llama Stack: Running (3 models available)
UI Status: Fully functional
External Access: 
  - UI: https://ui-rhai-agentic-demo.apps.rosa.aistackonrosa.611c.p3.openshiftapps.com
  - PDF Files: https://pdf-files-rhai-agentic-demo.apps.rosa.aistackonrosa.611c.p3.openshiftapps.com
```

**Total Resources:**
- **Compute**: 64 vCPUs, 128GB RAM
- **GPU**: 2x NVIDIA A10G (48GB VRAM total)
- **Deployment Time**: ~75 minutes end-to-end
- **Final Status**: 100% functional agentic AI demo

---

### **üö® Troubleshooting Common Issues (Based on Actual Deployment Experience)**

#### **Issue 1: InferenceServices Missing GPU Resources (CRITICAL)**
**Symptoms:** 
- Model serving pods in CrashLoopBackOff or Pending state
- vLLM error: `RuntimeError: Failed to infer device type`
- Llama Stack connection error: `llama_stack_client.APIConnectionError`
- Models fail to load with GPU acceleration

**Root Cause:** The default agentic demo deployment creates InferenceServices without `nvidia.com/gpu` resource requests, preventing vLLM from detecting and using GPU devices.

**‚úÖ TESTED FIX:**
```bash
# Add GPU resource requests to both InferenceServices
oc patch inferenceservice granite32-8b -n rhai-agentic-demo --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

oc patch inferenceservice llama32-3b -n rhai-agentic-demo --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

# Wait for pods to restart and verify GPU detection
sleep 180
oc logs $(oc get pods -n rhai-agentic-demo | grep granite32-8b-predictor | grep Running | awk '{print $1}') \
  -n rhai-agentic-demo -c kserve-container | grep -i cuda
# Expected: "Automatically detected platform cuda"
```

**Prevention:** This fix is now integrated into the main deployment steps above.

---

#### **Issue 2: PDF Files URL Not Working (CRITICAL)**
**Symptoms:** 
- PDF files URL returns 404 Not Found
- URL `https://pdf-files-rhai-agentic-demo.apps.<cluster-domain>/` shows nginx 404 error
- PDF service appears to be running but files are not accessible

**Root Cause:** The default nginx configuration only serves files from `/files/` path, but users expect the root path to work.

**‚úÖ TESTED FIX:**
```bash
# Apply nginx configuration patch to add root path redirect
oc patch configmap nginx-pdf-config -n rhai-agentic-demo --type='merge' -p='{
  "data": {
    "nginx.conf": "pid /tmp/nginx.pid;\nerror_log /dev/stderr;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /dev/stdout;\n    \n    client_body_temp_path /tmp/client_temp;\n    proxy_temp_path       /tmp/proxy_temp_path;\n    fastcgi_temp_path     /tmp/fastcgi_temp;\n    uwsgi_temp_path       /tmp/uwsgi_temp;\n    scgi_temp_path        /tmp/scgi_temp;\n    \n    server {\n        listen 8080;\n        server_name _;\n        \n        # CRITICAL FIX: Redirect root to /files/ for better UX\n        location = / {\n            return 301 /files/;\n        }\n        \n        location /files/ {\n            alias /usr/share/nginx/html/files/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            \n            # Allow CORS\n            add_header Access-Control-Allow-Origin *;\n            add_header Access-Control-Allow-Methods \"GET, OPTIONS\";\n            add_header Access-Control-Allow-Headers \"DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range\";\n            \n            # Handle OPTIONS requests\n            if ($request_method = OPTIONS) {\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 1728000;\n                add_header Content-Type text/plain charset=UTF-8;\n                add_header Content-Length 0;\n                return 204;\n            }\n        }\n        \n        # Health check endpoint\n        location /health {\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}"
  }
}'

# Restart the deployment to apply the fix
oc rollout restart deployment/pdf-mcp-server -n rhai-agentic-demo
oc rollout status deployment/pdf-mcp-server -n rhai-agentic-demo --timeout=120s

# Verify the fix works
curl -k -I https://pdf-files-rhai-agentic-demo.apps.<cluster-domain>/
# Should return: HTTP/1.1 301 Moved Permanently with location header pointing to /files/
```

**Prevention:** This fix is now integrated into all deployment methods above.

---

#### **Issue 3: Secret Name Mismatches (CRITICAL)**
**Symptoms:** 
- Pods show `CreateContainerConfigError` with "secret not found" errors
- MCP server pods fail to start
- Environment variables not populated correctly

**Root Cause:** Default secret creation doesn't match the exact names expected by the deployment manifests.

**‚úÖ TESTED FIX:**
```bash
# Create secrets with exact names expected by deployments
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: slack-mcp-server-secrets
  namespace: rhai-agentic-demo
type: Opaque
stringData:
  SLACK_BOT_TOKEN: "xoxb-dummy-token-for-demo-purposes"
  SLACK_TEAM_ID: "T0000000000"
---
apiVersion: v1
kind: Secret
metadata:
  name: crm-mcp-server-secrets
  namespace: rhai-agentic-demo
type: Opaque
stringData:
  CRM_API_KEY: "dummy-crm-api-key"
  CRM_BASE_URL: "https://api.example-crm.com"
EOF

# Restart affected deployments
oc rollout restart deployment/slack-mcp-server -n rhai-agentic-demo
oc rollout restart deployment/crm-mcp-server -n rhai-agentic-demo
```

**Prevention:** This fix is now integrated into the main deployment steps above.

---

#### **Issue 4: PDF Route URL Configuration (CRITICAL)**
**Symptoms:** 
- PDF MCP server pods fail to start
- Environment variable PDF_ROUTE_URL contains placeholder values
- PDF generation fails due to incorrect route references

**Root Cause:** Default deployment contains placeholder values for PDF route URL that need to be replaced with actual cluster domain.

**‚úÖ TESTED FIX:**
```bash
# Auto-configure PDF route URL before deployment
CONSOLE_URL=$(oc whoami --show-console)
CLUSTER_DOMAIN=$(echo $CONSOLE_URL | cut -d'/' -f3 | sed 's/console-openshift-console\.apps\.//')
PDF_ROUTE_URL="https://pdf-files-rhai-agentic-demo.apps.${CLUSTER_DOMAIN}"

# Update PDF deployment with correct route URL
sed -i "s|https://pdf-files-NAMESPACE.apps.CLUSTER-DOMAIN.com|${PDF_ROUTE_URL}|g" kubernetes/mcp-servers/pdf/pdf-deployment.yaml

# Apply the updated deployment
oc apply -f kubernetes/mcp-servers/pdf/pdf-deployment.yaml
```

**Prevention:** This fix is now integrated into the main deployment steps above.

---

#### **Issue 2: OLM Marketplace Not Ready (CRITICAL)**
**Symptoms:** Subscriptions created but no InstallPlans generated, operators never install
```bash
# Diagnosis
oc get pods -n openshift-marketplace
# Expected: Should show catalog pods running

oc get catalogsource -n openshift-marketplace
# Expected: 4 catalog sources (certified-operators, community-operators, redhat-marketplace, redhat-operators)

# Solution: Wait for marketplace readiness
echo "Waiting for marketplace to be ready..."
timeout 600 bash -c 'until oc get pods -n openshift-marketplace | grep -q Running; do sleep 30; echo "Waiting..."; done'

# Alternative: Proceed with container-based deployment (Phase 5)
```

#### **Issue 2: NFD and GPU Operators Failed (CRITICAL)**
**Symptoms:** NFD or GPU operators show "Failed" status, `AllNamespaces InstallModeType not supported` error
```bash
# Diagnosis
oc get csv -n openshift-operators | grep -E "(nfd|gpu)"
# If operators appear here, they are in WRONG namespace

oc describe csv <operator-name> -n openshift-operators | grep -A 5 "UnsupportedOperatorGroup"
# Expected: Shows "AllNamespaces InstallModeType not supported" error

# Root Cause: NFD and GPU operators CANNOT use openshift-operators namespace

# Solution: Move operators to dedicated namespaces
echo "üîß Fixing NFD and GPU operators in dedicated namespaces..."

# Remove failed operators from openshift-operators
oc delete subscription nfd -n openshift-operators || echo "Not found"
oc delete subscription gpu-operator-certified -n openshift-operators || echo "Not found"

# Create dedicated namespaces with OperatorGroups
oc create namespace openshift-nfd || echo "Exists"
oc create namespace nvidia-gpu-operator || echo "Exists"

# Create OperatorGroups (CRITICAL - without these, operators won't work)
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-nfd
  namespace: openshift-nfd
spec:
  targetNamespaces:
  - openshift-nfd
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator
  namespace: nvidia-gpu-operator
spec:
  targetNamespaces:
  - nvidia-gpu-operator
EOF

# Reinstall operators in correct namespaces
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nfd
  namespace: openshift-nfd
spec:
  channel: stable
  installPlanApproval: Automatic
  name: nfd
  source: redhat-operators
  sourceNamespace: openshift-marketplace
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gpu-operator-certified
  namespace: nvidia-gpu-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: gpu-operator-certified
  source: certified-operators
  sourceNamespace: openshift-marketplace
EOF

# Wait and verify
sleep 90
oc get csv -n openshift-nfd | grep nfd
oc get csv -n nvidia-gpu-operator | grep gpu
# Both should show "Succeeded"
```

#### **Issue 3: Secret Name Mismatches**
**Symptoms:** Pods show `CreateContainerConfigError` with "secret not found"
```bash
# Diagnosis
oc describe pod <failing-pod> -n rhai-agentic-demo | grep -A 3 -B 3 "secret.*not found"

# Common mismatches found:
# - Created: slack-secret, Expected: slack-mcp-server-secrets
# - Created: generic names, Expected: component-specific names

# Solution: Create secrets with exact names expected by deployments
oc get deployment <failing-deployment> -o yaml | grep -A 10 -B 5 secretKeyRef
```

#### **Issue 4: Namespace Mismatch in Kustomize Overlays**
**Symptoms:** `Error: namespace "rhai-agentic-demo" not found`
```bash
# Diagnosis
grep -r "rhai-agentic-demo" kubernetes/deploy-demo/overlays/default/

# Solution: Use individual component deployment instead of overlays
# OR modify kustomize files to use correct namespace
```

#### **Issue 5: Missing envsubst Command**
**Symptoms:** `command not found: envsubst` during secret creation
```bash
# Solution 1: Install envsubst (if needed)
# On macOS: brew install gettext
# On RHEL/CentOS: yum install gettext

# Solution 2: Manual secret creation (recommended)
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: <secret-name>
  namespace: <namespace>
type: Opaque
stringData:
  KEY: "value"
EOF
```

#### **Issue 6: GPU Hardware Not Detected**
**Symptoms:** GPU workloads not scheduling, no nvidia.com/gpu resources
```bash
# Diagnosis: Verify actual GPU hardware
GPU_NODE=$(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge -o jsonpath='{.items[0].metadata.name}')
oc debug node/$GPU_NODE -- chroot /host lspci | grep -i nvidia

# Expected: "NVIDIA Corporation GA102GL [A10G]"
# If not found: Check instance type and region availability
```

#### **Issue 7: Pods Stuck in ContainerCreating**
**Symptoms:** Pods remain in ContainerCreating state for extended periods
```bash
# Diagnosis
oc describe pod <stuck-pod> -n rhai-agentic-demo

# Common causes and solutions:
# 1. Image pull issues: Check image registry accessibility
# 2. Volume mount issues: Verify PVC creation
# 3. Security context issues: Check SCC policies
# 4. Resource constraints: Check node capacity

# Quick fix for resource issues:
oc describe nodes | grep -A 10 "Allocated resources"
```

#### **Issue 8: Kustomize Overlay CRD Dependencies**
**Symptoms:** `no matches for kind "ServingRuntime"` or `"InferenceService"`
```bash
# Diagnosis: Missing OpenShift AI operators and CRDs
oc get crd | grep -E "(serving|inference)"

# Solution: Use individual component deployment instead
# Deploy only components that don't require OpenShift AI CRDs:
# - database, ui, mcp-servers, crm, llama-stack (container-based)
```

#### **Issue 9: Route Accessibility Problems**
**Symptoms:** Routes created but applications not accessible
```bash
# Diagnosis
oc get routes -n rhai-agentic-demo
oc describe route <route-name> -n rhai-agentic-demo

# Check backend service
oc get endpoints <service-name> -n rhai-agentic-demo

# Test internal connectivity
oc run test-pod --image=curlimages/curl --rm -it -- curl http://<service-name>:<port>/
```

#### **üîß Emergency Recovery Commands**

```bash
# Complete cleanup and restart
oc delete project rhai-agentic-demo
# Wait for project deletion
oc new-project rhai-agentic-demo

# Reset operator installations
oc delete subscription --all -n redhat-ods-operator
oc delete subscription --all -n openshift-nfd
oc delete subscription --all -n nvidia-gpu-operator

# Check cluster resource usage
oc adm top nodes
oc get events --all-namespaces --sort-by='.lastTimestamp' | tail -20
```

**üí° Key Lessons Learned:**
1. **Always verify marketplace readiness** before operator installation
2. **Check secret names** in deployment YAMLs before creating secrets
3. **Use individual component deployment** when kustomize overlays fail
4. **Verify GPU hardware** with debug node commands
5. **Container-based AI inference** works without formal OpenShift AI operators
6. **Wait for proper timing** - new clusters need time for OLM to be ready

This completes the comprehensive, experience-based CLI deployment guide.
```

#### **Phase 2: Install OpenShift AI Operator**

```bash
# 6. OpenShift AI Operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: rhods-operator
  namespace: redhat-ods-operator
spec:
  channel: fast
  installPlanApproval: Automatic
  name: rhods-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# Wait for OpenShift AI operator to be ready
oc wait --for=condition=AtLatestKnown subscription/rhods-operator -n redhat-ods-operator --timeout=300s
```

#### **Phase 3: Create DataScienceCluster**

```bash
# 7. Create DataScienceCluster with configurable components
oc apply -f - <<EOF
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
  namespace: redhat-ods-applications
spec:
  components:
    dashboard:
      managementState: Managed
    workbenches:
      managementState: Managed
    modelmeshserving:
      managementState: Managed
    datasciencepipelines:
      managementState: Managed
    kserve:
      managementState: Managed
      serving:
        managementState: Managed
        name: knative-serving
    codeflare:
      managementState: Managed
    ray:
      managementState: Managed
    trustyai:
      managementState: Managed
    modelregistry:
      managementState: Managed
EOF
```

#### **Phase 4: Enable Model Catalog (Technology Preview)**

**Important:** Enable the model catalog feature for model registration, deployment, and customization capabilities.

```bash
echo "üîß Enabling Model Catalog feature..."

# Verify Model Registry component is enabled (should be enabled by default)
echo "üìä Verifying Model Registry component status:"
oc get datasciencecluster default-dsc -n redhat-ods-applications -o yaml | grep -A 3 -B 3 "model-registry-operator"

# Enable Model Catalog by setting disableModelCatalog to false
echo "üìä Configuring dashboard to enable Model Catalog:"
oc patch odhdashboardconfig odh-dashboard-config -n redhat-ods-applications \
  --type='merge' -p='{"spec":{"dashboardConfig":{"disableModelCatalog":false}}}'

# Verify the configuration change
echo "üìä Verifying Model Catalog configuration:"
oc get odhdashboardconfig odh-dashboard-config -n redhat-ods-applications \
  -o jsonpath='{.spec.dashboardConfig}' && echo ""

echo "‚úÖ Model Catalog enabled successfully!"
echo "üåê Navigate to 'Models' ‚Üí 'Model catalog' in the OpenShift AI dashboard"
```

---

## üìä **Monitoring and Validation Commands**

### **Check Operator Installation Status**
```bash
# Check all installed operators
oc get csv -A | grep -E "(rhods|nfd|gpu|serverless|servicemesh|authorino)"

# Check operator pods status
oc get pods -n redhat-ods-operator
oc get pods -n openshift-nfd
oc get pods -n nvidia-gpu-operator
oc get pods -n openshift-serverless
```

### **Check DataScienceCluster Status**
```bash
# Check DataScienceCluster resource
oc get datasciencecluster -n redhat-ods-applications

# Check detailed status
oc describe datasciencecluster default-dsc -n redhat-ods-applications

# Check OpenShift AI pods
oc get pods -n redhat-ods-applications
```

### **Check GPU Support (If Enabled)**
```bash
# Check GPU nodes
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true

# Check GPU resources
oc describe node <gpu-node-name> | grep nvidia.com/gpu

# Check NVIDIA device plugin
oc get pods -n nvidia-gpu-operator | grep nvidia-device-plugin
```

### **Access OpenShift AI Dashboard**
```bash
# Get dashboard route
oc get route rhods-dashboard -n redhat-ods-applications

# Expected URL format:
# https://rhods-dashboard-redhat-ods-applications.apps.<cluster-domain>
```

---

## üö® **CRITICAL AGENT INSTRUCTIONS**

### **MANDATORY AGENT BEHAVIOR**

1. **STRICT INPUT ADHERENCE**: 
   - NEVER alter or substitute user inputs automatically
   - NEVER change deployment method without explicit user permission
   - NEVER proceed with fallback options without user approval
   - NEVER continue deployment if prerequisites are missing

2. **ERROR HANDLING PROTOCOL**:
   - STOP EXECUTION IMMEDIATELY upon encountering any error
   - Refer back to this document for guidance FIRST
   - Identify the specific issue and root cause
   - Present user with clear options and recommendations
   - WAIT for explicit user decision before proceeding
   - Only continue after user provides clear direction

3. **PARAMETER VALIDATION**:
   - Validate EVERY parameter against this document's specifications
   - Search internet for validation rules not present in this document
   - Cite authoritative sources for any external validation
   - Present validation results to user for confirmation

### **MANDATORY ERROR HANDLING PROTOCOL**

When ANY error occurs during deployment:

1. **STOP EXECUTION** - Do not attempt automatic fixes
2. **DOCUMENT ERROR** - Capture exact error message and context
3. **REFER TO THIS DOCUMENT** - Check troubleshooting section first
4. **PRESENT OPTIONS** - Provide user with multiple resolution paths
5. **AWAIT DECISION** - Do not proceed without explicit user approval

#### **CONTEXT PRESERVATION SUMMARY**
When STOP EXECUTION is triggered, provide this summary:

```
üõë **DEPLOYMENT STOPPED - CONTEXT PRESERVATION**

**Current Status**: [Describe what was completed successfully]
**Error Encountered**: [Exact error message and context]
**Recommended Actions**: 
1. [Primary recommendation with rationale]
2. [Alternative approach with trade-offs]
3. [Fallback option if applicable]

**Next Steps**: Please review the options above and provide explicit direction on how to proceed.
**Documentation Reference**: Refer to OpenShift AI setup.md section [X] for detailed guidance.
```

### **EMERGENCY STOP CONDITIONS**
- Prerequisites not met and cannot be resolved
- User-specified parameters are invalid and cannot be corrected
- Deployment method fails and user rejects alternatives
- Critical cluster resources are insufficient
- Network connectivity issues prevent operator installation

---

## üîß **Troubleshooting Common Issues**

### **Issue 1: Operator Installation Failures**
```
ERR: Failed to install operator X
```
**Solutions**:
1. Check operator catalog availability: `oc get packagemanifest`
2. Verify namespace exists: `oc get namespace <namespace>`
3. Check subscription status: `oc describe subscription <name> -n <namespace>`
4. Review operator logs: `oc logs -n <namespace> -l app=<operator>`

### **Issue 2: DataScienceCluster Not Ready**
```
ERR: DataScienceCluster in failed state
```
**Solutions**:
1. Check cluster resource requirements
2. Verify all prerequisite operators are running
3. Check DataScienceCluster events: `oc describe datasciencecluster`
4. Review OpenShift AI operator logs

### **Issue 3: GPU Support Not Working**
```
ERR: GPU resources not available
```
**Solutions**:
1. Verify GPU nodes exist: `oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true`
2. Check NVIDIA GPU Operator status: `oc get pods -n nvidia-gpu-operator`
3. Verify node feature discovery: `oc get nodefeatures`
4. Check GPU device plugin logs

### **Issue 4: Dashboard Access Issues**
```
ERR: Cannot access OpenShift AI dashboard
```
**Solutions**:
1. Check route exists: `oc get route -n redhat-ods-applications`
2. Verify pods are running: `oc get pods -n redhat-ods-applications`
3. Check authentication configuration
4. Verify network policies and firewall rules

---

## üìã **Success Validation Checklist**

### **‚úÖ Deployment Success Criteria**

1. **Operators Installed**:
   - [ ] Node Feature Discovery Operator (if GPU enabled)
   - [ ] NVIDIA GPU Operator (if GPU enabled)
   - [ ] OpenShift Serverless Operator
   - [ ] OpenShift Service Mesh Operator
   - [ ] Authorino Operator
   - [ ] OpenShift AI Operator

2. **DataScienceCluster Status**:
   - [ ] DataScienceCluster resource exists
   - [ ] All components in "Managed" state
   - [ ] All pods running in redhat-ods-applications namespace

3. **Dashboard Access**:
   - [ ] Route accessible
   - [ ] Authentication working
   - [ ] Dashboard loads successfully

4. **GPU Support (if enabled)**:
   - [ ] GPU nodes detected
   - [ ] NVIDIA device plugin running
   - [ ] GPU resources available for scheduling

### **üß™ Post-Deployment Testing and Validation**

#### **Test 1: Create a Data Science Project**
```bash
# Test project creation via CLI
oc apply -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: test-data-science-project
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    openshift.io/display-name: 'Test Data Science Project'
    openshift.io/description: 'Test project for OpenShift AI validation'
EOF

# Verify project appears in dashboard
echo "Visit: https://rhods-dashboard-redhat-ods-applications.apps.<cluster-domain>"
echo "Check if 'Test Data Science Project' appears in the projects list"
```

#### **Test 2: Validate GPU Workbench Creation (If GPU Enabled)**
```bash
# Create a test GPU workbench
oc apply -f - <<EOF
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  name: test-gpu-workbench
  namespace: test-data-science-project
spec:
  template:
    spec:
      containers:
      - name: notebook
        image: quay.io/opendatahub/workbench-images:cuda-jupyter-minimal-ubi9-python-3.9-2024a-20240301
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 4Gi
            cpu: 1
          limits:
            nvidia.com/gpu: 1
            memory: 8Gi
            cpu: 2
EOF

# Check if workbench starts successfully
oc get notebook test-gpu-workbench -n test-data-science-project -w
```

#### **Test 3: Validate Model Serving Platform**
```bash
# Check KServe installation
oc get pods -n knative-serving | grep -E "(controller|webhook|activator)"

# Check ModelMesh installation  
oc get pods -n redhat-ods-applications | grep modelmesh

# Verify serving runtimes are available
oc get servingruntimes -n test-data-science-project
```

#### **Test 4: Validate Data Science Pipelines**
```bash
# Check pipeline components
oc get pods -n redhat-ods-applications | grep -E "(pipelines|workflow)"

# Test pipeline creation
oc apply -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: test-pipeline
  namespace: test-data-science-project
spec:
  tasks:
  - name: hello-world
    taskSpec:
      steps:
      - name: echo
        image: registry.redhat.io/ubi8/ubi-minimal
        script: |
          echo "OpenShift AI Pipeline Test Successful"
EOF
```

### **üìä Success Response Template**

```
üéâ **OPENSHIFT AI DEPLOYMENT SUCCESSFUL**

**Cluster Information**:
- **Cluster Name**: <cluster-name>
- **OpenShift Version**: <version>
- **Deployment Method**: <terraform|cli>
- **GPU Support**: <enabled|disabled>

**Deployed Components**:
- ‚úÖ OpenShift AI Dashboard
- ‚úÖ Data Science Workbenches
- ‚úÖ Model Mesh Serving
- ‚úÖ Data Science Pipelines
- ‚úÖ KServe Model Serving
- ‚úÖ CodeFlare Distributed Computing
- ‚úÖ Ray Distributed Computing
- ‚úÖ TrustyAI Explainability
- ‚úÖ Model Registry

**Access Information**:
- **Dashboard URL**: https://rhods-dashboard-redhat-ods-applications.apps.<cluster-domain>
- **Login Method**: Use your OpenShift cluster credentials
- **Admin Access**: cluster-admin role required for full functionality

**GPU Resources** (if enabled):
- **GPU Nodes**: <gpu-node-count>
- **GPU Type**: <gpu-model>
- **Available GPUs**: <gpu-count>

**Validation Tests Completed**:
- ‚úÖ Data Science Project Creation
- ‚úÖ Dashboard Access and Authentication
- ‚úÖ Workbench Image Availability
- ‚úÖ Model Serving Platform Readiness
- ‚úÖ Pipeline Execution Capability
- ‚úÖ GPU Resource Detection (if applicable)

**Next Steps**:
1. Access the OpenShift AI dashboard using the URL above
2. Create your first data science project
3. Launch a Jupyter notebook workbench
4. Explore available notebook images and tools
5. Configure model serving endpoints as needed
6. Set up data connections (S3, databases, etc.)

**Workshop Resources**:
- [Getting Started Tutorial](https://redhatquickcourses.github.io/llm-on-rhoai/)
- [LLM Deployment Guide](https://cloud.redhat.com/experts/rhoai/rosa-s3/)
- [GPU Workload Examples](https://cloud.redhat.com/experts/rhoai/rosa-gpu/)

**Documentation**: 
- OpenShift AI User Guide: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed
- Getting Started: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/getting_started_with_red_hat_openshift_ai_self-managed

**Support**: For issues, refer to OpenShift AI setup.md troubleshooting section.
```

---

## üóëÔ∏è **Cleanup and Uninstallation**

### **Terraform Cleanup**
```bash
# Destroy OpenShift AI components
terraform destroy -target=null_resource.create_openshift_ai_application
terraform destroy -target=null_resource.create_nvidia_gpu_gitops_application

# Or destroy all
terraform destroy
```

### **CLI Cleanup**
```bash
# Delete DataScienceCluster
oc delete datasciencecluster default-dsc -n redhat-ods-applications

# Delete operators (in reverse order)
oc delete subscription rhods-operator -n redhat-ods-operator
oc delete subscription authorino-operator -n openshift-operators
oc delete subscription servicemeshoperator -n openshift-operators
oc delete subscription serverless-operator -n openshift-serverless
oc delete subscription gpu-operator-certified -n nvidia-gpu-operator
oc delete subscription nfd -n openshift-nfd

# Delete namespaces
oc delete namespace redhat-ods-applications
oc delete namespace redhat-ods-operator
oc delete namespace nvidia-gpu-operator
oc delete namespace openshift-nfd
```

---

## üéì **Workshop and Hands-On Resources**

### **Red Hat Official Workshops**
- [OpenShift AI Quick Courses](https://redhatquickcourses.github.io/llm-on-rhoai/) - Comprehensive learning path
- [LLM on Red Hat OpenShift AI](https://redhatquickcourses.github.io/llm-on-rhoai/model-serving/1.1/chapter2/rhoai_install_guide.html) - Installation guide
- [Deploy Red Hat OpenShift AI on AWS](https://aws.amazon.com/blogs/ibm-redhat/deploy-red-hat-openshift-ai-on-aws-for-scalable-ai-ml-solutions/) - AWS-specific guide

### **Practical Examples and Use Cases**
- [Running and Deploying LLMs using RHOAI on ROSA with S3](https://cloud.redhat.com/experts/rhoai/rosa-s3/) - LLM deployment tutorial
- [Creating Images using Stable Diffusion on ROSA with GPU](https://cloud.redhat.com/experts/rhoai/rosa-gpu/) - GPU-accelerated AI workloads
- [Model Serving on Single-Model Platform](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service/installing-and-deploying-openshift-ai_install) - Model deployment patterns

### **Advanced Configuration Examples**

#### **DataScienceCluster Advanced Configuration**
```yaml
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: advanced-dsc
  namespace: redhat-ods-applications
spec:
  components:
    dashboard:
      managementState: Managed
    workbenches:
      managementState: Managed
    modelmeshserving:
      managementState: Managed
      devFlags:
        manifests:
          - uri: https://github.com/opendatahub-io/modelmesh-serving/tarball/release-0.11.0
    datasciencepipelines:
      managementState: Managed
    kserve:
      managementState: Managed
      serving:
        managementState: Managed
        name: knative-serving
        ingressGateway:
          certificate:
            type: SelfSigned
    codeflare:
      managementState: Managed
    ray:
      managementState: Managed
    trustyai:
      managementState: Managed
      devFlags:
        manifests:
          - uri: https://github.com/trustyai-explainability/trustyai-service-operator/tarball/main
    modelregistry:
      managementState: Managed
      registriesNamespace: odh-model-registries
```

#### **GPU-Optimized Workbench Configuration**
```yaml
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  name: gpu-workbench
  namespace: my-data-science-project
spec:
  template:
    spec:
      containers:
      - name: notebook
        image: quay.io/opendatahub/workbench-images:cuda-jupyter-minimal-ubi9-python-3.9-2024a-20240301
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 8Gi
            cpu: 2
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: all
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: compute,utility
```

### **Object Storage Configuration for S3**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: aws-connection-my-storage
  namespace: my-data-science-project
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: My S3 Storage
stringData:
  AWS_ACCESS_KEY_ID: <your-access-key>
  AWS_SECRET_ACCESS_KEY: <your-secret-key>
  AWS_S3_ENDPOINT: https://s3.amazonaws.com
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_BUCKET: my-data-science-bucket
```

## üìö **Additional Resources**

### **Official Documentation**
- [Red Hat OpenShift AI Documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed)
- [OpenShift AI Installation Guide](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/installing_and_uninstalling_openshift_ai_self-managed)
- [OpenShift AI Cloud Service Guide](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service)

### **Technical References**
- [GPU Support Configuration](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/openshift/index.html)
- [Terraform OpenShift Provider](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs)
- [Open Data Hub Community](https://opendatahub.io/) - Upstream project documentation

### **Community and Support**
- [Red Hat Developer Hub](https://developers.redhat.com/products/openshift-ai) - Developer resources
- [OpenShift AI Community](https://github.com/opendatahub-io) - GitHub repositories
- [Red Hat Customer Portal](https://access.redhat.com/) - Support and knowledge base

### **Agentic AI References**
- **[Red Hat Agentic AI Demo](https://github.com/rh-aiservices-bu/rhai-agentic-demo)** - Production-ready agentic AI implementation
- [Llama Stack GitHub](https://github.com/meta-llama/llama-stack) - Official Llama Stack repository
- [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) - Tool integration framework

---

## üöÄ **Advanced OpenShift AI Capabilities**

### **LLM Performance Evaluation with GuideLLM**

GuideLLM is Red Hat's official toolkit for evaluating and optimizing LLM deployments on OpenShift AI. It provides comprehensive performance metrics and optimization recommendations.

#### **What is GuideLLM?**
- **Official Red Hat Tool**: Primary framework for benchmarking customer models
- **vLLM Integration**: Built into vLLM upstream project for seamless performance evaluation  
- **Real-World Simulation**: Simulates actual inference workloads for accurate assessment
- **Cost Optimization**: Provides insights for efficient resource utilization and cost management

#### **Key Performance Metrics**

| Metric | Description | Target Values | Business Impact |
|--------|-------------|---------------|-----------------|
| **TTFT** (Time to First Token) | Latency for first response token | <200ms interactive, <500ms productivity | User experience and responsiveness |
| **ITL** (Inter-Token Latency) | Time between subsequent tokens | <50ms for smooth streaming | Streaming quality and engagement |
| **Throughput** | Requests processed per second | Varies by use case | Scalability and concurrent user support |
| **Request Latency** | End-to-end request processing time | P99 <1000ms for production | SLA compliance and reliability |

#### **GuideLLM Deployment Methods**

##### **Method 1: Tekton Pipeline (Recommended for Production)**

```bash
# Install GuideLLM Tekton components
oc apply -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: guidellm-benchmark-task
  namespace: openshift-ai
spec:
  params:
  - name: target
    description: Target endpoint for benchmarking
  - name: model-name
    description: Model name for evaluation
  - name: data-config
    description: Token configuration (e.g., prompt_tokens=512,output_tokens=256)
  - name: rate-type
    description: Benchmark type (synchronous, throughput, constant, poisson, sweep)
    default: "sweep"
  - name: max-seconds
    description: Maximum benchmark duration
    default: "30"
  steps:
  - name: benchmark
    image: ghcr.io/neuralmagic/guidellm:latest
    script: |
      #!/bin/bash
      guidellm \
        --target $(params.target) \
        --model $(params.model-name) \
        --data-type emulated \
        --data-config "$(params.data-config)" \
        --rate-type $(params.rate-type) \
        --max-seconds $(params.max-seconds) \
        --output-path /workspace/results/benchmark-results.json
    workingDir: /workspace
    volumeMounts:
    - name: results-volume
      mountPath: /workspace/results
  volumes:
  - name: results-volume
    emptyDir: {}
EOF

# Create benchmark pipeline
oc apply -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: guidellm-benchmark-pipeline
  namespace: openshift-ai
spec:
  params:
  - name: target
    description: Target endpoint for benchmarking
  - name: model-name
    description: Model name for evaluation
  - name: data-config
    description: Token configuration
    default: "prompt_tokens=512,output_tokens=256"
  - name: rate-type
    description: Benchmark type
    default: "sweep"
  tasks:
  - name: benchmark-model
    taskRef:
      name: guidellm-benchmark-task
    params:
    - name: target
      value: $(params.target)
    - name: model-name
      value: $(params.model-name)
    - name: data-config
      value: $(params.data-config)
    - name: rate-type
      value: $(params.rate-type)
EOF
```

##### **Method 2: CLI Tool**

```bash
# Install GuideLLM CLI
pip install guidellm

# Run performance evaluation
guidellm \
  --target http://your-model-endpoint:8080/v1 \
  --model your-model-name \
  --data-type emulated \
  --data-config "prompt_tokens=512,output_tokens=256" \
  --rate-type sweep \
  --max-seconds 300 \
  --output-path benchmark-results.json
```

#### **Performance Evaluation Scenarios**

##### **Chat Applications (512/256 tokens)**
```bash
# Optimized for conversational AI
tkn pipeline start guidellm-benchmark-pipeline \
  --param target=http://model-endpoint:8080/v1 \
  --param model-name=granite-8b-chat \
  --param data-config="prompt_tokens=512,output_tokens=256" \
  --param rate-type=constant
```

##### **RAG Applications (4096/512 tokens)**
```bash
# Optimized for document retrieval and generation
tkn pipeline start guidellm-benchmark-pipeline \
  --param target=http://model-endpoint:8080/v1 \
  --param model-name=granite-8b-rag \
  --param data-config="prompt_tokens=4096,output_tokens=512" \
  --param rate-type=poisson
```

##### **Code Generation (512/512 tokens)**
```bash
# Optimized for code assistant applications
tkn pipeline start guidellm-benchmark-pipeline \
  --param target=http://model-endpoint:8080/v1 \
  --param model-name=granite-8b-code \
  --param data-config="prompt_tokens=512,output_tokens=512" \
  --param rate-type=sweep
```

#### **Terraform Integration for GuideLLM**

```terraform
# GuideLLM Tekton Pipeline Deployment
resource "null_resource" "deploy_guidellm_pipeline" {
  count = var.deploy_guidellm ? 1 : 0
  depends_on = [null_resource.create_openshift_ai_application]

  provisioner "local-exec" {
    command = <<EOF
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Deploy GuideLLM Tekton components
      oc apply -f - <<GUIDELLM_EOF
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: guidellm-benchmark-task
  namespace: redhat-ods-applications
spec:
  params:
  - name: target
    description: Target endpoint for benchmarking
  - name: model-name
    description: Model name for evaluation
  - name: data-config
    description: Token configuration
    default: "prompt_tokens=512,output_tokens=256"
  - name: rate-type
    description: Benchmark type
    default: "sweep"
  steps:
  - name: benchmark
    image: ghcr.io/neuralmagic/guidellm:latest
    script: |
      #!/bin/bash
      guidellm --target \$(params.target) --model \$(params.model-name) --data-type emulated --data-config "\$(params.data-config)" --rate-type \$(params.rate-type) --max-seconds 300 --output-path /workspace/results/benchmark-results.json
    volumeMounts:
    - name: results-volume
      mountPath: /workspace/results
  volumes:
  - name: results-volume
    emptyDir: {}
GUIDELLM_EOF

      echo "GuideLLM pipeline deployed successfully"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_guidellm = var.deploy_guidellm
  }
}

variable "deploy_guidellm" {
  type        = bool
  default     = false
  description = "Deploy GuideLLM performance evaluation pipeline"
}
```

---

## ü¶ô **Llama Stack Integration**

Llama Stack provides a comprehensive framework for agentic AI operations and advanced LLM capabilities on OpenShift AI.

#### **What is Llama Stack?**
- **Agentic AI Platform**: Framework for building autonomous AI agents
- **Tool Integration**: Seamless integration with external tools and APIs
- **Memory Management**: Persistent context and conversation memory
- **Multi-Modal Support**: Text, image, and code processing capabilities

#### **Container Image Sources**

**üî¥ CRITICAL: Choose the correct image based on your deployment needs**

##### **Red Hat's Official Recommendation**

According to [Red Hat OpenShift AI Documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.22/html/working_with_rag/deploying-a-rag-stack-in-a-data-science-project_rag), Red Hat provides an optimized Llama Stack image specifically designed for OpenShift AI integration.

**üéØ Production Deployment Path:**
1. **Developer Preview**: `quay.io/opendatahub/llama-stack:odh`
2. **General Availability**: Will move to `registry.redhat.io` (planned)
3. **Support**: Full Red Hat support and validation
4. **Integration**: Native OpenShift AI ecosystem compatibility

##### **Option 1: Red Hat Official Image (RECOMMENDED for Production)**
```yaml
image: quay.io/opendatahub/llama-stack:odh
```
- **Source**: Red Hat OpenDataHub on Quay.io
- **Status**: Developer Preview (GA planned for registry.redhat.io)
- **Best For**: Production OpenShift AI deployments
- **Support**: Red Hat validated and supported
- **Integration**: Optimized for OpenShift AI ecosystem

##### **Option 2: Meta Official Image (Alternative)**
```yaml
image: llamastack/distribution-meta-reference-gpu
```
- **Source**: Meta's official Docker Hub repository
- **Status**: Upstream community image
- **Best For**: Development, testing, or upstream compatibility
- **Support**: Community supported
- **Integration**: Standard Meta Llama Stack implementation

##### **‚ö†Ô∏è DEPRECATED/INCORRECT IMAGES**
```yaml
# ‚ùå DO NOT USE - These images don't exist:
image: llamastack/llamastack:latest
image: llamastack/llamastack:stable
```

#### **Llama Stack Deployment**

##### **üéØ RECOMMENDED: Official Build Approach (VALIDATED)**

**‚úÖ VALIDATED APPROACH:** The official Llama Stack build command has been successfully tested and validated.

**Prerequisites:**
```bash
# Install Llama Stack CLI
pip3 install llama-stack --user

# Install uv for package management
curl -LsSf https://astral.sh/uv/install.sh | sh
export PATH="$HOME/.local/bin:$PATH"
```

**Build Command:**
```bash
# Create working directory
mkdir -p llama-stack-build && cd llama-stack-build

# Build Llama Stack with GPU support
llama stack build --distro starter-gpu --image-type venv --image-name custom-llama-stack-gpu

# Run the built distribution
llama stack run ~/.llama/distributions/starter-gpu/starter-gpu-run.yaml --image-type venv --image-name custom-llama-stack-gpu
```

**‚úÖ Validation Results:**
- **API Endpoints**: ‚úÖ Working (`/v1/models` returns comprehensive provider list)
- **vLLM Support**: ‚úÖ Included (configurable via `VLLM_URL` environment variable)
- **Multi-Provider**: ‚úÖ Supports Fireworks, OpenAI, Anthropic, Gemini, Groq, SambaNova, etc.
- **Production Ready**: ‚úÖ Full REST API with health checks
- **GPU Optimized**: ‚úÖ starter-gpu distribution includes GPU-specific optimizations

**Key Features:**
- **vLLM Integration**: Built-in vLLM provider support
- **Environment Configuration**: All providers configurable via environment variables
- **Vector Databases**: Supports Faiss, ChromaDB, Milvus, PGVector
- **Safety**: Includes Llama Guard and code scanning capabilities
- **Tool Runtime**: Supports web search, RAG, and MCP integrations

**Container Deployment on OpenShift:**
```bash
# For OpenShift deployment, create a custom container image
# Note: Requires Docker/Podman for container builds
llama stack build --distro starter-gpu --image-type container --image-name llamastack-gpu-openshift

# Deploy to OpenShift (use the generated container image)
oc new-app llamastack-gpu-openshift --name=llama-stack-official
```

##### **CLI Deployment (Legacy Approach)**

```bash
# Create namespace for Llama Stack
oc create namespace llama-stack

# Deploy Llama Stack components
oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-server
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack-server
  template:
    metadata:
      labels:
        app: llama-stack-server
    spec:
      containers:
      - name: llama-stack
        image: quay.io/opendatahub/llama-stack:odh  # Red Hat official image (recommended)
        ports:
        - containerPort: 8080
        env:
        - name: LLAMA_STACK_CONFIG
          value: "/config/stack-config.yaml"
        volumeMounts:
        - name: config-volume
          mountPath: /config
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
      volumes:
      - name: config-volume
        configMap:
          name: llama-stack-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  stack-config.yaml: |
    inference:
      provider: vllm
      config:
        model: meta-llama/Llama-3.2-8B-Instruct
        tensor_parallel_size: 1
    agents:
      provider: meta-reference
      config:
        persistence_store:
          provider: redis
          config:
            host: redis-service
            port: 6379
    memory:
      provider: redis
      config:
        host: redis-service
        port: 6379
    safety:
      provider: llama-guard
      config:
        model: meta-llama/Llama-Guard-3-8B
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-service
  namespace: llama-stack
spec:
  selector:
    app: llama-stack-server
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
EOF

# Deploy Redis for memory persistence
oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: llama-stack
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
EOF
```

##### **Terraform Deployment Options**

###### **Option A: Official Build Approach (RECOMMENDED)**

```terraform
# Llama Stack Official Build Deployment
resource "null_resource" "deploy_llama_stack_official" {
  count = var.deploy_llama_stack_official ? 1 : 0
  depends_on = [null_resource.create_openshift_ai_application]

  provisioner "local-exec" {
    command = <<EOF
      # Install Llama Stack CLI if not present
      pip3 install llama-stack --user || true
      curl -LsSf https://astral.sh/uv/install.sh | sh || true
      export PATH="$HOME/.local/bin:/Users/$(whoami)/Library/Python/3.13/bin:$PATH"

      # Create build directory
      mkdir -p /tmp/llama-stack-terraform-build
      cd /tmp/llama-stack-terraform-build

      # Build Llama Stack with GPU support
      llama stack build --distro starter-gpu --image-type venv --image-name terraform-llama-stack-gpu

      # Create OpenShift deployment from built distribution
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Create namespace
      oc create namespace llama-stack || true

      # Deploy using the official build
      cat > llama-stack-deployment.yaml <<LLAMA_DEPLOYMENT_EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-official
  namespace: llama-stack
  labels:
    app: llama-stack-official
    deployment-method: official-build
spec:
  replicas: ${var.llama_stack_replicas}
  selector:
    matchLabels:
      app: llama-stack-official
  template:
    metadata:
      labels:
        app: llama-stack-official
    spec:
      containers:
      - name: llama-stack
        image: python:3.13-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install llama-stack --user
          curl -LsSf https://astral.sh/uv/install.sh | sh
          export PATH="/root/.local/bin:$PATH"
          cd /tmp && mkdir -p llama-build && cd llama-build
          llama stack build --distro starter-gpu --image-type venv --image-name runtime-llama-stack
          llama stack run ~/.llama/distributions/starter-gpu/starter-gpu-run.yaml --image-type venv --image-name runtime-llama-stack --port 8080
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: VLLM_URL
          value: "${var.vllm_endpoint_url}"
        - name: OPENAI_API_KEY
          value: "${var.openai_api_key}"
        resources:
          requests:
            memory: ${var.llama_stack_memory}
            cpu: ${var.llama_stack_cpu}
            ${var.llama_stack_gpu_count > 0 ? "nvidia.com/gpu: \"${var.llama_stack_gpu_count}\"" : ""}
          limits:
            memory: ${var.llama_stack_memory_limit}
            cpu: ${var.llama_stack_cpu_limit}
            ${var.llama_stack_gpu_count > 0 ? "nvidia.com/gpu: \"${var.llama_stack_gpu_count}\"" : ""}
        ${var.llama_stack_gpu_count > 0 ? "nodeSelector:\n        node-type: gpu" : ""}
        ${var.llama_stack_gpu_count > 0 ? "tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule" : ""}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000810000
          fsGroup: 1000810000
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-official-service
  namespace: llama-stack
spec:
  selector:
    app: llama-stack-official
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
LLAMA_DEPLOYMENT_EOF

      oc apply -f llama-stack-deployment.yaml
      echo "Official Llama Stack build deployed successfully"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_llama_stack_official = var.deploy_llama_stack_official
  }
}

# Variables for Official Build Approach
variable "deploy_llama_stack_official" {
  type        = bool
  default     = true
  description = "Deploy Llama Stack using official build approach (RECOMMENDED)"
}

variable "vllm_endpoint_url" {
  type        = string
  default     = ""
  description = "vLLM endpoint URL for inference (optional)"
}

variable "openai_api_key" {
  type        = string
  default     = ""
  description = "OpenAI API key for inference (optional)"
  sensitive   = true
}
```

###### **Option B: Legacy Container Approach**

```terraform
# Llama Stack Legacy Deployment with configurable image source
resource "null_resource" "deploy_llama_stack_legacy" {
  count = var.deploy_llama_stack_legacy ? 1 : 0
  depends_on = [null_resource.create_openshift_ai_application]

  provisioner "local-exec" {
    command = <<EOF
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Create Llama Stack namespace
      oc create namespace llama-stack || true

      # Deploy Llama Stack components (Legacy approach)
      oc apply -f - <<LLAMA_STACK_EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-server
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack-server
  template:
    metadata:
      labels:
        app: llama-stack-server
    spec:
      containers:
      - name: llama-stack
        image: ${var.llama_stack_image}
        ports:
        - containerPort: 8080
        env:
        - name: LLAMA_STACK_CONFIG
          value: "/config/stack-config.yaml"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: "${var.llama_stack_gpu_count}"
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "${var.llama_stack_gpu_count}"
LLAMA_STACK_EOF

      echo "Llama Stack deployed successfully"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_llama_stack = var.deploy_llama_stack
  }
}

# Terraform Variables for Llama Stack Configuration
variable "deploy_llama_stack" {
  type        = bool
  default     = false
  description = "Deploy Llama Stack for agentic AI operations"
}

variable "llama_stack_image" {
  type        = string
  default     = "quay.io/opendatahub/llama-stack:odh"
  description = "Llama Stack container image. Options: 'quay.io/opendatahub/llama-stack:odh' (Red Hat official) or 'llamastack/distribution-meta-reference-gpu' (Meta official)"
}

variable "llama_stack_gpu_count" {
  type        = number
  default     = 1
  description = "Number of GPUs for Llama Stack deployment"
}

# Example terraform.tfvars configuration:
# deploy_llama_stack = true
# llama_stack_image = "quay.io/opendatahub/llama-stack:odh"  # Red Hat official (recommended)
# # OR
# llama_stack_image = "llamastack/distribution-meta-reference-gpu"  # Meta official
```

#### **Agentic AI Operations with Llama Stack**

##### **System Administration Agent**

```python
# Example: System monitoring agent
import requests
import json

def create_monitoring_agent():
    agent_config = {
        "agent_id": "system-monitor",
        "model": "meta-llama/Llama-3.2-8B-Instruct",
        "instructions": """
        You are a system administration agent for OpenShift AI.
        Monitor cluster health, resource usage, and alert on anomalies.
        Use the provided tools to gather system information and take corrective actions.
        """,
        "tools": [
            {
                "name": "get_cluster_status",
                "description": "Get OpenShift cluster status and health metrics"
            },
            {
                "name": "scale_deployment",
                "description": "Scale deployments based on resource usage"
            },
            {
                "name": "send_alert",
                "description": "Send alerts to administrators"
            }
        ]
    }
    
    response = requests.post(
        "http://llama-stack-service:8080/agents",
        headers={"Content-Type": "application/json"},
        data=json.dumps(agent_config)
    )
    return response.json()
```

---

## üåê **Model-as-a-Service (MaaS) Infrastructure**

Comprehensive MaaS implementation using OpenShift AI, 3scale API Gateway, and enterprise integration patterns.

#### **MaaS Architecture Components**

1. **Model Serving Layer**: OpenShift AI + KServe/ModelMesh
2. **API Gateway**: 3scale for authentication, rate limiting, and analytics
3. **Load Balancing**: OpenShift Service Mesh for traffic management
4. **Monitoring**: Prometheus + Grafana for observability
5. **Security**: OAuth2/OIDC with Red Hat SSO integration

#### **MaaS Deployment**

##### **CLI Deployment**

```bash
# Deploy 3scale API Gateway
oc create namespace 3scale-system

# Install 3scale operator
oc apply -f - <<EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: 3scale-operator
  namespace: 3scale-system
spec:
  channel: threescale-2.15
  installPlanApproval: Automatic
  name: 3scale-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

# Create API Manager instance
oc apply -f - <<EOF
apiVersion: apps.3scale.net/v1alpha1
kind: APIManager
metadata:
  name: apimanager-sample
  namespace: 3scale-system
spec:
  wildcardDomain: apps.your-cluster-domain.com
  resourceRequirementsEnabled: true
  backend:
    listenerSpec:
      resources:
        requests:
          memory: "550Mi"
          cpu: "500m"
        limits:
          memory: "700Mi"
          cpu: "1000m"
  system:
    appSpec:
      resources:
        requests:
          memory: "600Mi"
          cpu: "500m"
        limits:
          memory: "800Mi"
          cpu: "1000m"
EOF

# Deploy model serving endpoint
oc apply -f - <<EOF
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-8b-maas
  namespace: redhat-ods-applications
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      runtime: vllm
      resources:
        requests:
          nvidia.com/gpu: 1
          memory: 16Gi
          cpu: 4
        limits:
          nvidia.com/gpu: 1
          memory: 32Gi
          cpu: 8
      env:
      - name: MODEL_NAME
        value: ibm-granite/granite-3.3-8b-instruct
      - name: MAX_MODEL_LEN
        value: "8192"
      - name: GPU_MEMORY_UTILIZATION
        value: "0.9"
EOF

# Create API Product in 3scale
oc apply -f - <<EOF
apiVersion: capabilities.3scale.net/v1beta1
kind: Product
metadata:
  name: granite-8b-api
  namespace: 3scale-system
spec:
  name: "Granite 8B Model API"
  systemName: "granite-8b-api"
  description: "Enterprise LLM API powered by Granite 8B"
  metrics:
    hits:
      friendlyName: "Hits"
      unit: "hit"
  methods:
    generate:
      friendlyName: "Generate"
      systemName: "generate"
  applicationPlans:
    basic:
      name: "Basic Plan"
      limits:
      - period: "minute"
        value: 100
        metricMethodRef:
          systemName: "hits"
    premium:
      name: "Premium Plan"
      limits:
      - period: "minute"
        value: 1000
        metricMethodRef:
          systemName: "hits"
  backends:
  - name: "granite-8b-backend"
    systemName: "granite-8b-backend"
    privateBaseURL: "http://granite-8b-maas-predictor.redhat-ods-applications.svc.cluster.local"
EOF
```

##### **Terraform Deployment**

```terraform
# MaaS Infrastructure Deployment
resource "null_resource" "deploy_maas_infrastructure" {
  count = var.deploy_maas ? 1 : 0
  depends_on = [null_resource.create_openshift_ai_application]

  provisioner "local-exec" {
    command = <<EOF
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Deploy 3scale operator
      oc create namespace 3scale-system || true
      
      oc apply -f - <<THREESCALE_EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: 3scale-operator
  namespace: 3scale-system
spec:
  channel: threescale-2.15
  installPlanApproval: Automatic
  name: 3scale-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
THREESCALE_EOF

      # Wait for operator to be ready
      sleep 60

      # Deploy API Manager
      oc apply -f - <<APIMANAGER_EOF
apiVersion: apps.3scale.net/v1alpha1
kind: APIManager
metadata:
  name: apimanager-sample
  namespace: 3scale-system
spec:
  wildcardDomain: ${var.cluster_domain}
  resourceRequirementsEnabled: true
APIMANAGER_EOF

      echo "MaaS infrastructure deployed successfully"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_maas = var.deploy_maas
  }
}

variable "deploy_maas" {
  type        = bool
  default     = false
  description = "Deploy Model-as-a-Service infrastructure with 3scale"
}

variable "cluster_domain" {
  type        = string
  description = "Cluster wildcard domain for 3scale configuration"
}
```

#### **API Gateway Integration**

##### **Rate Limiting Configuration**

```yaml
apiVersion: capabilities.3scale.net/v1beta1
kind: ApplicationPlan
metadata:
  name: enterprise-plan
  namespace: 3scale-system
spec:
  appsRequireApproval: false
  trialPeriod: 0
  limits:
  - period: "minute"
    value: 1000
    metricMethodRef:
      systemName: "hits"
  - period: "hour"
    value: 10000
    metricMethodRef:
      systemName: "hits"
  - period: "day"
    value: 100000
    metricMethodRef:
      systemName: "hits"
  pricingRules:
  - from: 1
    to: 1000
    pricePerUnit: "0.01"
    metricMethodRef:
      systemName: "hits"
```

##### **Authentication Configuration**

```yaml
apiVersion: capabilities.3scale.net/v1beta1
kind: Backend
metadata:
  name: granite-8b-backend
  namespace: 3scale-system
spec:
  name: "Granite 8B Backend"
  systemName: "granite-8b-backend"
  privateBaseURL: "http://granite-8b-maas-predictor.redhat-ods-applications.svc.cluster.local"
  mappingRules:
  - httpMethod: "POST"
    pattern: "/v1/chat/completions"
    metricMethodRef: "generate"
    increment: 1
  - httpMethod: "POST"
    pattern: "/v1/completions"
    metricMethodRef: "generate"
    increment: 1
```

---

## üíª **Code Assistant Implementation**

Enterprise code assistant powered by OpenShift AI with IDE integration and advanced code generation capabilities.

#### **Code Assistant Architecture**

1. **Model Layer**: Fine-tuned code generation models (Granite Code, CodeLlama)
2. **API Layer**: RESTful API with OpenAI-compatible endpoints
3. **IDE Integration**: VS Code extension, IntelliJ plugin, web-based editor
4. **Context Management**: Repository indexing and semantic search
5. **Security**: Code scanning and compliance checking

#### **Code Assistant Deployment**

##### **CLI Deployment**

```bash
# Deploy code generation model
oc apply -f - <<EOF
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-code-assistant
  namespace: redhat-ods-applications
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      runtime: vllm
      resources:
        requests:
          nvidia.com/gpu: 1
          memory: 16Gi
          cpu: 4
        limits:
          nvidia.com/gpu: 1
          memory: 32Gi
          cpu: 8
      env:
      - name: MODEL_NAME
        value: ibm-granite/granite-3.3-8b-code-instruct
      - name: MAX_MODEL_LEN
        value: "8192"
      - name: GPU_MEMORY_UTILIZATION
        value: "0.9"
      - name: SERVED_MODEL_NAME
        value: granite-code
EOF

# Deploy code assistant API gateway
oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-assistant-gateway
  namespace: redhat-ods-applications
spec:
  replicas: 2
  selector:
    matchLabels:
      app: code-assistant-gateway
  template:
    metadata:
      labels:
        app: code-assistant-gateway
    spec:
      containers:
      - name: gateway
        image: quay.io/rh-aiservices-bu/code-assistant-gateway:latest
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_ENDPOINT
          value: "http://granite-code-assistant-predictor.redhat-ods-applications.svc.cluster.local"
        - name: API_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: code-assistant-secrets
              key: api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: code-assistant-service
  namespace: redhat-ods-applications
spec:
  selector:
    app: code-assistant-gateway
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: code-assistant-route
  namespace: redhat-ods-applications
spec:
  to:
    kind: Service
    name: code-assistant-service
  port:
    targetPort: 8080
  tls:
    termination: edge
EOF

# Create API key secret
oc create secret generic code-assistant-secrets \
  --from-literal=api-key=$(openssl rand -hex 32) \
  -n redhat-ods-applications
```

##### **Terraform Deployment**

```terraform
# Code Assistant Infrastructure
resource "null_resource" "deploy_code_assistant" {
  count = var.deploy_code_assistant ? 1 : 0
  depends_on = [null_resource.create_openshift_ai_application]

  provisioner "local-exec" {
    command = <<EOF
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Generate API key
      API_KEY=$(openssl rand -hex 32)
      
      # Create secrets
      oc create secret generic code-assistant-secrets \
        --from-literal=api-key=$API_KEY \
        -n redhat-ods-applications || true

      # Deploy code generation model
      oc apply -f - <<CODE_MODEL_EOF
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-code-assistant
  namespace: redhat-ods-applications
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      runtime: vllm
      resources:
        requests:
          nvidia.com/gpu: ${var.code_assistant_gpu_count}
          memory: 16Gi
          cpu: 4
        limits:
          nvidia.com/gpu: ${var.code_assistant_gpu_count}
          memory: 32Gi
          cpu: 8
      env:
      - name: MODEL_NAME
        value: ${var.code_model_name}
      - name: MAX_MODEL_LEN
        value: "8192"
CODE_MODEL_EOF

      echo "Code assistant deployed successfully"
      echo "API Key: $API_KEY"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_code_assistant = var.deploy_code_assistant
  }
}

variable "deploy_code_assistant" {
  type        = bool
  default     = false
  description = "Deploy code assistant with Granite Code model"
}

variable "code_assistant_gpu_count" {
  type        = number
  default     = 1
  description = "Number of GPUs for code assistant deployment"
}

variable "code_model_name" {
  type        = string
  default     = "ibm-granite/granite-3.3-8b-code-instruct"
  description = "Code generation model name"
}
```

#### **VS Code Extension Integration**

```typescript
// VS Code extension configuration
import * as vscode from 'vscode';

export class OpenShiftAICodeAssistant {
    private apiEndpoint: string;
    private apiKey: string;

    constructor() {
        const config = vscode.workspace.getConfiguration('openshift-ai');
        this.apiEndpoint = config.get('endpoint', 'https://code-assistant-route-redhat-ods-applications.apps.your-cluster.com');
        this.apiKey = config.get('apiKey', '');
    }

    async generateCode(prompt: string, language: string): Promise<string> {
        const response = await fetch(`${this.apiEndpoint}/v1/completions`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`
            },
            body: JSON.stringify({
                model: 'granite-code',
                prompt: `Language: ${language}\n\n${prompt}`,
                max_tokens: 512,
                temperature: 0.1,
                stop: ['\n\n']
            })
        });

        const data = await response.json();
        return data.choices[0].text;
    }
}
```

#### **Interactive Code Game Implementation**

```python
# Code challenge system
class CodeChallengeSystem:
    def __init__(self, model_endpoint, api_key):
        self.endpoint = model_endpoint
        self.api_key = api_key
        
    def generate_challenge(self, difficulty="medium", topic="algorithms"):
        prompt = f"""
        Generate a {difficulty} difficulty coding challenge about {topic}.
        Include:
        1. Problem description
        2. Input/output examples
        3. Constraints
        4. Starter code template
        
        Format as JSON with keys: description, examples, constraints, template
        """
        
        response = requests.post(
            f"{self.endpoint}/v1/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "granite-code",
                "prompt": prompt,
                "max_tokens": 1024,
                "temperature": 0.7
            }
        )
        
        return response.json()["choices"][0]["text"]
    
    def evaluate_solution(self, challenge, solution):
        prompt = f"""
        Evaluate this coding solution:
        
        Challenge: {challenge}
        Solution: {solution}
        
        Provide feedback on:
        1. Correctness
        2. Efficiency
        3. Code quality
        4. Suggestions for improvement
        """
        
        response = requests.post(
            f"{self.endpoint}/v1/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "granite-code",
                "prompt": prompt,
                "max_tokens": 512,
                "temperature": 0.3
            }
        )
        
        return response.json()["choices"][0]["text"]
```

---

## üìä **Analytics & Monitoring**

Comprehensive monitoring and analytics for all OpenShift AI capabilities using Prometheus, Grafana, and custom metrics.

#### **Monitoring Stack Deployment**

```bash
# Deploy monitoring components
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-openshift-ai
  namespace: openshift-monitoring
data:
  openshift-ai-dashboard.json: |
    {
      "dashboard": {
        "title": "OpenShift AI Performance",
        "panels": [
          {
            "title": "Model Inference Latency",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(model_inference_duration_seconds_bucket[5m]))",
                "legendFormat": "P95 Latency"
              }
            ]
          },
          {
            "title": "GPU Utilization",
            "type": "graph",
            "targets": [
              {
                "expr": "nvidia_gpu_utilization_percent",
                "legendFormat": "GPU {{gpu}}"
              }
            ]
          },
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total[5m])",
                "legendFormat": "Requests/sec"
              }
            ]
          }
        ]
      }
    }
EOF
```

---

## ü§ñ **Production Agentic AI Demo Deployment**

After successfully deploying OpenShift AI with Llama Stack, you can deploy Red Hat's production-ready agentic AI demonstration that showcases real-world enterprise integrations.

### **What is the Red Hat Agentic AI Demo?**

The [Red Hat Agentic AI Demo](https://github.com/rh-aiservices-bu/rhai-agentic-demo) is a comprehensive production-ready implementation that demonstrates:

- **Multi-Agent Workflows**: Autonomous AI agents working together
- **Enterprise Integrations**: CRM, PDF generation, Slack, and process reports
- **Tool Orchestration**: MCP (Model Context Protocol) servers for external system integration
- **Real-world Use Cases**: Account management, support case analysis, and automated reporting

#### **Demo Architecture Overview**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   UI Frontend   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ  Llama Stack     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   MCP Servers   ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ    Server        ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ - User Interface‚îÇ    ‚îÇ - Agent Orchestr.‚îÇ    ‚îÇ - CRM Integration‚îÇ
‚îÇ - Request/Resp. ‚îÇ    ‚îÇ - LLM Management ‚îÇ    ‚îÇ - PDF Generation ‚îÇ
‚îÇ - Real-time UI  ‚îÇ    ‚îÇ - Tool Selection ‚îÇ    ‚îÇ - Slack Bot      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ - Process Reports‚îÇ
                                ‚ñ≤               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ      LLMs       ‚îÇ
                       ‚îÇ                 ‚îÇ
                       ‚îÇ - Granite 3.2-8B‚îÇ
                       ‚îÇ - Llama 3.2-3B  ‚îÇ
                       ‚îÇ - GPU Accelerated‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Prerequisites Validation**

Before deploying the agentic AI demo, ensure these components are operational:

#### **CLI Validation Commands**

```bash
# 1. Verify OpenShift AI is running
oc get pods -n redhat-ods-applications | grep -E "(dashboard|notebook|pipeline)"

# 2. Check GPU operator status
oc get pods -n nvidia-gpu-operator | grep nvidia-device-plugin

# 3. Verify Node Feature Discovery
oc get pods -n openshift-nfd | grep nfd

# 4. Check GPU nodes are available
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true

# 5. Verify Llama Stack deployment (if using our module)
oc get pods -n llama-stack | grep llama-stack-server

# 6. Check cluster resources (minimum 2 GPUs with 24GiB VRAM each)
oc describe nodes | grep -A 5 "nvidia.com/gpu"
```

#### **Terraform Validation**

```bash
# Check deployment status
terraform output openshift_ai_enabled
terraform output llama_stack_enabled
terraform output nvidia_gpu_operator_gitops_enabled

# Verify resource allocation
terraform output llama_stack_resource_requirements
```

### **Demo Deployment Methods**

#### **Method 1: Quick Deployment (Recommended) - WITH ALL PRODUCTION FIXES**

```bash
# Set up environment
export NAMESPACE=rhai-agentic-demo
oc create namespace $NAMESPACE

# ‚ö†Ô∏è CRITICAL: Pre-deployment validation
echo "üîç Validating GPU prerequisites..."
GPU_NODES=$(oc get nodes -l node.kubernetes.io/instance-type=g5.2xlarge --no-headers | wc -l)
if [ "$GPU_NODES" -lt 2 ]; then
    echo "‚ùå ERROR: Need at least 2 GPU nodes, found: $GPU_NODES"
    exit 1
fi

# Clone the demo repository
git clone https://github.com/rh-aiservices-bu/rhai-agentic-demo.git
cd rhai-agentic-demo

# CRITICAL FIX 1: Create secrets with exact names expected by deployments
echo "üîß Creating required secrets with correct names..."
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: slack-mcp-server-secrets
  namespace: $NAMESPACE
type: Opaque
stringData:
  SLACK_BOT_TOKEN: "xoxb-dummy-token-for-demo-purposes"
  SLACK_TEAM_ID: "T0000000000"
---
apiVersion: v1
kind: Secret
metadata:
  name: crm-mcp-server-secrets
  namespace: $NAMESPACE
type: Opaque
stringData:
  CRM_API_KEY: "dummy-crm-api-key"
  CRM_BASE_URL: "https://api.example-crm.com"
EOF

# CRITICAL FIX 2: Auto-configure PDF route URL before deployment
echo "üîß Configuring PDF route URL..."
CONSOLE_URL=$(oc whoami --show-console)
CLUSTER_DOMAIN=$(echo $CONSOLE_URL | cut -d'/' -f3 | sed 's/console-openshift-console\.apps\.//')
PDF_ROUTE_URL="https://pdf-files-${NAMESPACE}.apps.${CLUSTER_DOMAIN}"
sed -i "s|https://pdf-files-NAMESPACE.apps.CLUSTER-DOMAIN.com|${PDF_ROUTE_URL}|g" kubernetes/mcp-servers/pdf/pdf-deployment.yaml

# Deploy with local GPU inference (default)
echo "üöÄ Deploying Red Hat Agentic AI Demo..."
oc apply -k kubernetes/deploy-demo/overlays/default

# Wait for initial deployment
echo "‚è≥ Waiting for initial deployment (120 seconds)..."
sleep 120

# CRITICAL FIX 3: Add GPU resource requests to InferenceServices
echo "üîß CRITICAL FIX: Adding GPU resource requests to InferenceServices..."
sleep 30  # Wait for InferenceServices to be created

oc patch inferenceservice granite32-8b -n ${NAMESPACE} --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

oc patch inferenceservice llama32-3b -n ${NAMESPACE} --type='merge' \
  -p='{"spec":{"predictor":{"model":{"resources":{"requests":{"nvidia.com/gpu":"1"},"limits":{"nvidia.com/gpu":"1"}}}}}}'

echo "‚úÖ GPU resource requests added to both InferenceServices"

# CRITICAL FIX 4: Apply nginx configuration fix for PDF files URL accessibility
echo "üîß CRITICAL FIX: Updating nginx configuration for PDF accessibility..."
sleep 60  # Wait for PDF MCP server to be deployed

oc patch configmap nginx-pdf-config -n ${NAMESPACE} --type='merge' -p='{
  "data": {
    "nginx.conf": "pid /tmp/nginx.pid;\nerror_log /dev/stderr;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /dev/stdout;\n    \n    client_body_temp_path /tmp/client_temp;\n    proxy_temp_path       /tmp/proxy_temp_path;\n    fastcgi_temp_path     /tmp/fastcgi_temp;\n    uwsgi_temp_path       /tmp/uwsgi_temp;\n    scgi_temp_path        /tmp/scgi_temp;\n    \n    server {\n        listen 8080;\n        server_name _;\n        \n        # CRITICAL FIX: Redirect root to /files/ for better UX\n        location = / {\n            return 301 /files/;\n        }\n        \n        location /files/ {\n            alias /usr/share/nginx/html/files/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            \n            # Allow CORS\n            add_header Access-Control-Allow-Origin *;\n            add_header Access-Control-Allow-Methods \"GET, OPTIONS\";\n            add_header Access-Control-Allow-Headers \"DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range\";\n            \n            # Handle OPTIONS requests\n            if ($request_method = OPTIONS) {\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 1728000;\n                add_header Content-Type text/plain charset=UTF-8;\n                add_header Content-Length 0;\n                return 204;\n            }\n        }\n        \n        # Health check endpoint\n        location /health {\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}"
  }
}'

# Restart PDF MCP server to apply nginx configuration
oc rollout restart deployment/pdf-mcp-server -n ${NAMESPACE}
oc rollout status deployment/pdf-mcp-server -n ${NAMESPACE} --timeout=120s

# Wait for model serving pods to restart with GPU access
echo "‚è≥ Waiting for model serving pods to restart with GPU resources (180 seconds)..."
sleep 180

# Final validation and URL display
echo "üîç FINAL DEPLOYMENT VALIDATION:"
oc get pods -n ${NAMESPACE}
oc get inferenceservice -n ${NAMESPACE}

echo -e "\nüåê Application Access URLs:"
UI_URL=$(oc get route ui -n ${NAMESPACE} -o jsonpath='{.spec.host}' 2>/dev/null)
PDF_URL=$(oc get route pdf-files -n ${NAMESPACE} -o jsonpath='{.spec.host}' 2>/dev/null)

if [ ! -z "$UI_URL" ]; then
    echo "‚úÖ UI Dashboard: https://$UI_URL"
else
    echo "‚ùå UI Dashboard: Not available"
fi

if [ ! -z "$PDF_URL" ]; then
    echo "‚úÖ PDF Service: https://$PDF_URL (with redirect fix applied)"
else
    echo "‚ùå PDF Service: Not available"
fi

echo -e "\nüéâ AGENTIC AI DEMO DEPLOYMENT COMPLETED WITH ALL FIXES!"
echo "‚úÖ InferenceServices GPU resource allocation applied"
echo "‚úÖ PDF nginx configuration with root path redirect applied"
echo "‚úÖ Secret names corrected for deployment compatibility"
echo "‚úÖ PDF route URL auto-configuration applied"

# Alternative: Deploy with Models-as-a-Service (no local GPUs required)
# oc apply -k kubernetes/deploy-demo/overlays/maas
```

#### **Method 2: Terraform Integration**

Add this to your terraform configuration to deploy the demo automatically:

```terraform
# Red Hat Agentic AI Demo Deployment
resource "null_resource" "deploy_agentic_ai_demo" {
  count = var.deploy_agentic_ai_demo ? 1 : 0
  depends_on = [module.llama_stack]

  provisioner "local-exec" {
    command = <<EOF
      # Login to cluster
      oc login --username="${module.rosa_cluster_hcp.cluster_admin_username}" --password="${module.rosa_cluster_hcp.cluster_admin_password}" "${module.rosa_cluster_hcp.cluster_api_url}" --insecure-skip-tls-verify

      # Set up demo namespace
      oc create namespace rhai-agentic-demo || true

      # Clone and configure demo
      if [ ! -d "/tmp/rhai-agentic-demo" ]; then
        git clone https://github.com/rh-aiservices-bu/rhai-agentic-demo.git /tmp/rhai-agentic-demo
      fi
      cd /tmp/rhai-agentic-demo

      # Configure Slack (use provided credentials)
      export SLACK_BOT_TOKEN="${var.slack_bot_token}"
      export SLACK_TEAM_ID="${var.slack_team_id}"
      envsubst < kubernetes/mcp-servers/slack/slack-secret.yaml | oc apply -f -

      # Auto-configure PDF route
      CONSOLE_URL=$(oc whoami --show-console)
      CLUSTER_DOMAIN=$(echo $CONSOLE_URL | cut -d'/' -f3 | sed 's/console-openshift-console\.apps\.//')
      PDF_ROUTE_URL="https://pdf-files-rhai-agentic-demo.apps.$CLUSTER_DOMAIN"
      sed -i "s|https://pdf-files-NAMESPACE.apps.CLUSTER-DOMAIN.com|$PDF_ROUTE_URL|g" kubernetes/mcp-servers/pdf/pdf-deployment.yaml

      # Deploy demo
      oc apply -k kubernetes/deploy-demo/overlays/default

      # Apply nginx configuration fix for PDF files URL accessibility
      echo "Applying nginx configuration fix for PDF files URL..."
      oc patch configmap nginx-pdf-config -n rhai-agentic-demo --type='merge' -p='{
        "data": {
          "nginx.conf": "pid /tmp/nginx.pid;\nerror_log /dev/stderr;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    access_log /dev/stdout;\n    \n    client_body_temp_path /tmp/client_temp;\n    proxy_temp_path       /tmp/proxy_temp_path;\n    fastcgi_temp_path     /tmp/fastcgi_temp;\n    uwsgi_temp_path       /tmp/uwsgi_temp;\n    scgi_temp_path        /tmp/scgi_temp;\n    \n    server {\n        listen 8080;\n        server_name _;\n        \n        # Redirect root to /files/ (FIXES PDF URL ACCESSIBILITY)\n        location = / {\n            return 301 /files/;\n        }\n        \n        location /files/ {\n            alias /usr/share/nginx/html/files/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            \n            # Allow CORS\n            add_header Access-Control-Allow-Origin *;\n            add_header Access-Control-Allow-Methods \"GET, OPTIONS\";\n            add_header Access-Control-Allow-Headers \"DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range\";\n            \n            # Handle OPTIONS requests\n            if ($request_method = OPTIONS) {\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 1728000;\n                add_header Content-Type text/plain charset=UTF-8;\n                add_header Content-Length 0;\n                return 204;\n            }\n        }\n        \n        # Health check endpoint\n        location /health {\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}"
        }
      }'

      # Restart PDF MCP server to apply the fix
      oc rollout restart deployment/pdf-mcp-server -n rhai-agentic-demo
      oc rollout status deployment/pdf-mcp-server -n rhai-agentic-demo --timeout=120s

      echo "Agentic AI demo deployed successfully with PDF URL fix!"
    EOF
  }

  triggers = {
    cluster_id = module.rosa_cluster_hcp.cluster_id
    deploy_demo = var.deploy_agentic_ai_demo
  }
}

variable "deploy_agentic_ai_demo" {
  type        = bool
  default     = false
  description = "Deploy Red Hat Agentic AI demo"
}

variable "slack_bot_token" {
  type        = string
  default     = ""
  description = "Slack bot token for demo integration"
  sensitive   = true
}

variable "slack_team_id" {
  type        = string
  default     = ""
  description = "Slack team ID for demo integration"
}
```

### **Slack Integration Setup**

The demo requires Slack integration for real-world enterprise communication workflows.

#### **Step 1: Create Slack App**

1. Go to [Slack API Apps](https://api.slack.com/apps)
2. Click **"Create New App"** ‚Üí **"From scratch"**
3. Name your app (e.g., "RHAI Agentic Demo")
4. Select your workspace

#### **Step 2: Configure Bot Permissions**

1. Go to **"OAuth & Permissions"**
2. Add these Bot Token Scopes:
   - `channels:read`
   - `chat:write`
   - `chat:write.public`
   - `files:write`
   - `groups:read`
   - `im:read`
   - `mpim:read`

#### **Step 3: Get Credentials**

1. **Bot Token**: Copy from "OAuth & Permissions" ‚Üí "Bot User OAuth Token" (starts with `xoxb-`)
2. **Team ID**: Found in workspace URL or "Basic Information" page

#### **Step 4: Install App to Workspace**

1. Go to **"Install App"**
2. Click **"Install to Workspace"**
3. Authorize the app

### **Demo Validation and Testing**

#### **Check Deployment Status**

```bash
# Verify all components are running
oc get pods -n rhai-agentic-demo

# Check services and routes
oc get svc,route -n rhai-agentic-demo

# Get demo UI URL
oc get route -n rhai-agentic-demo ui --template='https://{{.spec.host}}{{"\n"}}'

# Check Llama Stack server logs
oc logs -f deployment/llama-stack-server -n rhai-agentic-demo

# Verify MCP servers
oc get pods -n rhai-agentic-demo | grep mcp
```

#### **Test the Demo**

Access the demo UI and try these sample requests:

```
1. Review the current opportunities for ACME corp
2. Get a list of support cases for the account
3. Determine the status of the account (happy/unhappy) based on the cases
4. Generate a PDF document with a summary of the support cases and account status
5. Send a summary of the account plus a link to download the PDF via Slack to the general channel
```

### **Demo Components Deep Dive**

#### **MCP (Model Context Protocol) Servers**

The demo uses MCP servers for enterprise system integration:

| MCP Server | Purpose | Integration |
|------------|---------|-------------|
| **CRM Server** | Customer data management | Salesforce, HubSpot APIs |
| **PDF Server** | Document generation | Dynamic PDF creation |
| **Slack Server** | Team communication | Slack Bot API |
| **Process Server** | Workflow automation | Business process integration |

#### **LLM Models Deployed**

| Model | Purpose | Resource Requirements |
|-------|---------|---------------------|
| **Granite 3.2-8B** | General reasoning and orchestration | 1 GPU, 16Gi RAM |
| **Llama 3.2-3B** | Specialized task execution | 1 GPU, 8Gi RAM |

#### **Success Metrics**

After successful deployment, you should see:

- ‚úÖ **UI Accessible**: Demo interface loads without errors
- ‚úÖ **Slack Integration**: Bot responds in Slack channels  
- ‚úÖ **PDF Generation**: Documents created and downloadable
- ‚úÖ **Multi-turn Conversations**: Context maintained across requests
- ‚úÖ **CRM Integration**: Account data retrieved and processed
- ‚úÖ **Performance**: Sub-3 second response times for most queries

This comprehensive agentic AI demo showcases the full potential of OpenShift AI with Llama Stack for enterprise autonomous AI applications, providing a foundation for building production-ready intelligent automation systems.

---

This comprehensive addition transforms the OpenShift AI setup document into a complete enterprise AI platform guide, covering performance evaluation, agentic operations, model-as-a-service, code assistance capabilities, and now production-ready agentic AI demonstrations. All components are designed to work seamlessly with both CLI and Terraform deployment methods, following the same structured approach as the original document.

---

## ü¶ô **Llama Stack Integration with OpenShift AI (Post-Deployment Configuration)**

After successful OpenShift AI deployment via Terraform, follow these steps to configure Llama Stack for seamless integration with OpenShift AI. This enables advanced agentic AI capabilities, model serving, and enterprise-grade AI workloads.

### **üìã Integration Prerequisites**

Before proceeding, ensure these components are deployed and ready:

```bash
# Verify OpenShift AI is fully deployed
oc get datasciencecluster default-dsc -n redhat-ods-operator

# Check Llama Stack Operator status
oc get pods -n redhat-ods-applications | grep llama-stack-k8s-operator

# Verify GPU resources are available
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true

# Confirm required CRDs are available
oc get crd | grep llama
```

**‚úÖ Expected Status:**
- DataScienceCluster: `Ready` with `llamastackoperator: managementState: Managed`
- Llama Stack Operator: `1/1 Running`
- GPU Nodes: Available with Tesla T4 or compatible GPUs
- CRDs: `llamastackdistributions.llamastack.io` available

### **üîß Step 1: Deploy LlamaStackDistribution Instance**

Create and deploy the core Llama Stack components:

```yaml
# Create: llama-stack-distribution.yaml
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: default-llama-stack
  namespace: redhat-ods-applications
  labels:
    app: llama-stack
    integration: openshift-ai
spec:
  # Use starter-gpu distribution for GPU-enabled deployments
  distribution: "starter-gpu"
  
  # Red Hat's official OpenShift AI optimized image
  image:
    repository: "quay.io/opendatahub/llama-stack"
    tag: "odh"
    pullPolicy: "IfNotPresent"
  
  # Resource requirements optimized for Tesla T4 GPUs
  resources:
    requests:
      memory: "8Gi"
      cpu: "4"
      nvidia.com/gpu: "1"
    limits:
      memory: "16Gi"
      cpu: "8" 
      nvidia.com/gpu: "1"
  
  # Target GPU nodes with proper tolerations
  nodeSelector:
    node-type: gpu
  
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  
  # Persistent storage for model cache and data
  storage:
    size: "50Gi"
    mountPath: "/data"
  
  # Environment configuration for OpenShift AI integration
  env:
  - name: LLAMA_STACK_CONFIG
    value: "/config/stack-config.yaml"
  - name: PYTHONUNBUFFERED
    value: "1"
  - name: LOG_LEVEL
    value: "INFO"
  - name: OPENSHIFT_AI_INTEGRATION
    value: "true"
  
  # Service configuration
  service:
    type: ClusterIP
    ports:
    - name: http
      port: 8080
      targetPort: 8080
    - name: metrics
      port: 9090
      targetPort: 9090
```

**Deploy the instance:**
```bash
# Apply the LlamaStackDistribution configuration
oc apply -f llama-stack-distribution.yaml

# Monitor deployment progress
oc get llamastackdistributions -n redhat-ods-applications -w

# Check pod status
oc get pods -n redhat-ods-applications | grep default-llama-stack
```

### **üîß Step 2: Configure Model Serving Runtime (vLLM Integration)**

Set up vLLM serving runtime for high-performance model inference:

```yaml
# Create: vllm-serving-runtime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-llama-runtime
  namespace: redhat-ods-applications
  labels:
    app: llama-stack
    component: model-serving
spec:
  supportedModelFormats:
  - name: pytorch
    version: "1"
    autoSelect: true
  - name: huggingface
    version: "1"
    autoSelect: true
  
  containers:
  - name: kserve-container
    image: quay.io/opendatahub/vllm:stable
    args:
    - --model
    - /mnt/models
    - --port
    - "8080"
    - --served-model-name
    - llama-model
    - --trust-remote-code
    - --gpu-memory-utilization
    - "0.85"
    - --max-model-len
    - "4096"
    
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: "16Gi"
        nvidia.com/gpu: "1"
    
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP
    
    env:
    - name: CUDA_VISIBLE_DEVICES
      value: "0"
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
  
  builtInAdapter:
    serverType: vllm
    runtimeManagementPort: 8080
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 300000
```

**Deploy the serving runtime:**
```bash
# Apply vLLM serving runtime
oc apply -f vllm-serving-runtime.yaml

# Verify serving runtime is available
oc get servingruntimes -n redhat-ods-applications
```

### **üîß Step 3: Configure Authentication and Security**

Set up OAuth/OIDC authentication for secure API access:

```yaml
# Create: llama-stack-auth-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-auth-config
  namespace: redhat-ods-applications
  labels:
    app: llama-stack
    component: authentication
data:
  auth-config.yaml: |
    server:
      auth:
        provider_config:
          type: "oauth2_token"
          jwks:
            uri: "https://kubernetes.default.svc:8443/openid/v1/jwks"
            token: "${env.SERVICE_ACCOUNT_TOKEN}"
            key_recheck_period: 3600
          validation:
            audience: "https://kubernetes.default.svc"
            issuer: "https://kubernetes.default.svc:8443"
            required_claims:
              sub: "system:serviceaccount:*"
        
        # API key fallback for development
        api_keys:
          enabled: true
          header_name: "X-API-Key"
          
    # CORS configuration for web integration
    cors:
      allow_origins:
      - "https://*.apps.*.openshiftapps.com"
      - "https://rhods-dashboard-*"
      allow_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
      allow_headers: ["Authorization", "Content-Type", "X-API-Key"]
```

**Apply authentication configuration:**
```bash
# Deploy authentication config
oc apply -f llama-stack-auth-config.yaml

# Create service account for Llama Stack
oc create serviceaccount llama-stack-service -n redhat-ods-applications

# Grant necessary permissions
oc adm policy add-cluster-role-to-user view system:serviceaccount:redhat-ods-applications:llama-stack-service
```

### **üîß Step 4: Create External Access Routes**

Expose Llama Stack APIs via OpenShift routes:

```yaml
# Create: llama-stack-routes.yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llama-stack-api
  namespace: redhat-ods-applications
  labels:
    app: llama-stack
    component: api
spec:
  to:
    kind: Service
    name: default-llama-stack-service
    weight: 100
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llama-stack-metrics
  namespace: redhat-ods-applications
  labels:
    app: llama-stack
    component: metrics
spec:
  to:
    kind: Service
    name: default-llama-stack-service
    weight: 100
  port:
    targetPort: metrics
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
```

**Deploy routes:**
```bash
# Create external access routes
oc apply -f llama-stack-routes.yaml

# Get API endpoint URLs
oc get routes -n redhat-ods-applications | grep llama-stack
```

### **üîß Step 5: Data Science Project Integration**

Create a dedicated project for Llama Stack model serving:

```bash
# Create Data Science Project for Llama Stack
oc new-project llama-stack-models

# Label project for OpenShift AI dashboard integration
oc label namespace llama-stack-models opendatahub.io/dashboard=true
oc label namespace llama-stack-models modelmesh-enabled=true

# Create model serving example
cat << EOF | oc apply -f -
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-2-8b-instruct
  namespace: llama-stack-models
  labels:
    app: llama-stack
    model: llama-3.2-8b
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface
      runtime: vllm-llama-runtime
      storage:
        key: default-bucket
        path: models/llama-3.2-8b-instruct
      resources:
        requests:
          cpu: "4"
          memory: "8Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: "16Gi"
          nvidia.com/gpu: "1"
EOF
```

### **üîß Step 6: Integration Validation and Testing**

Comprehensive validation of the Llama Stack OpenShift AI integration:

```bash
echo "=== LLAMA STACK OPENSHIFT AI INTEGRATION VALIDATION ==="

# 1. Check LlamaStackDistribution status
echo -e "\n1. Verifying LlamaStackDistribution deployment..."
oc get llamastackdistributions -n redhat-ods-applications
oc describe llamastackdistribution default-llama-stack -n redhat-ods-applications

# 2. Verify pods are running
echo -e "\n2. Checking Llama Stack pods status..."
oc get pods -n redhat-ods-applications | grep llama

# 3. Test API endpoints
echo -e "\n3. Testing API endpoints..."
LLAMA_API_URL=$(oc get route llama-stack-api -n redhat-ods-applications -o jsonpath='{.spec.host}')
echo "API URL: https://$LLAMA_API_URL"

# Health check
curl -k -s https://$LLAMA_API_URL/health | jq . || echo "Health endpoint not ready"

# Models endpoint
curl -k -s https://$LLAMA_API_URL/v1/models | jq . || echo "Models endpoint not ready"

# 4. Check model serving
echo -e "\n4. Verifying model serving integration..."
oc get servingruntimes -n redhat-ods-applications
oc get inferenceservice -n llama-stack-models

# 5. Verify routes and services
echo -e "\n5. Checking external access..."
oc get routes,services -n redhat-ods-applications | grep llama

# 6. Test OpenShift AI dashboard integration
echo -e "\n6. Validating OpenShift AI dashboard integration..."
RHODS_URL=$(oc get route rhods-dashboard -n redhat-ods-applications -o jsonpath='{.spec.host}')
echo "OpenShift AI Dashboard: https://$RHODS_URL"

# 7. Check GPU utilization
echo -e "\n7. GPU resource validation..."
oc get nodes -l feature.node.kubernetes.io/pci-0300_10de.present=true
```

### **‚úÖ Integration Success Criteria**

The integration is successful when all these conditions are met:

#### **Core Components:**
- [ ] LlamaStackDistribution instance: `Ready` status
- [ ] Llama Stack pods: `Running` (1/1 or more replicas)
- [ ] vLLM serving runtime: Available and configured
- [ ] Authentication: OAuth/OIDC configured and working

#### **External Access:**
- [ ] API route: Accessible via HTTPS
- [ ] Health endpoint: Returns 200 OK
- [ ] Models endpoint: Returns available models list
- [ ] Metrics endpoint: Prometheus metrics available

#### **OpenShift AI Integration:**
- [ ] Data Science Project: Created and labeled
- [ ] Dashboard integration: Llama Stack visible in RHODS dashboard
- [ ] Model serving: InferenceService deployed successfully
- [ ] GPU resources: Properly allocated and utilized

#### **Functionality Tests:**
- [ ] Model inference: API accepts and processes requests
- [ ] Authentication: Token validation working
- [ ] Monitoring: Metrics collection active
- [ ] Jupyter integration: Notebooks can connect to Llama Stack APIs

### **üö® Troubleshooting Common Issues**

#### **Issue 1: LlamaStackDistribution Stuck in Pending**
```bash
# Check operator logs
oc logs -n redhat-ods-applications deployment/llama-stack-k8s-operator-controller-manager

# Verify GPU node availability
oc get nodes -l node-type=gpu

# Check resource constraints
oc describe llamastackdistribution default-llama-stack -n redhat-ods-applications
```

#### **Issue 2: Pods Not Scheduling on GPU Nodes**
```bash
# Verify node selectors and tolerations
oc get pods -n redhat-ods-applications -o wide | grep llama

# Check GPU node taints
oc describe nodes | grep -A 5 -B 5 nvidia.com/gpu

# Verify GPU operator is running
oc get pods -n nvidia-gpu-operator
```

#### **Issue 3: API Endpoints Not Accessible**
```bash
# Check service endpoints
oc get endpoints -n redhat-ods-applications | grep llama

# Verify route configuration
oc describe route llama-stack-api -n redhat-ods-applications

# Test internal connectivity
oc run test-pod --image=curlimages/curl --rm -it -- curl http://default-llama-stack-service:8080/health
```

### **üìä Integration Monitoring and Maintenance**

#### **Regular Health Checks:**
```bash
# Daily health validation script
cat << 'EOF' > llama-stack-health-check.sh
#!/bin/bash
echo "=== Llama Stack Health Check ==="
echo "Date: $(date)"

# Check pod health
echo -e "\n1. Pod Status:"
oc get pods -n redhat-ods-applications | grep llama

# Check API health
echo -e "\n2. API Health:"
LLAMA_API_URL=$(oc get route llama-stack-api -n redhat-ods-applications -o jsonpath='{.spec.host}')
curl -k -s https://$LLAMA_API_URL/health

# Check GPU utilization
echo -e "\n3. GPU Utilization:"
oc exec -n redhat-ods-applications $(oc get pods -n redhat-ods-applications -l app=llama-stack -o jsonpath='{.items[0].metadata.name}') -- nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits

echo -e "\n=== Health Check Complete ==="
EOF

chmod +x llama-stack-health-check.sh
```

#### **Performance Monitoring:**
```bash
# Monitor resource usage
oc adm top pods -n redhat-ods-applications | grep llama

# Check metrics endpoint
METRICS_URL=$(oc get route llama-stack-metrics -n redhat-ods-applications -o jsonpath='{.spec.host}')
curl -k -s https://$METRICS_URL/metrics | grep -E "(gpu|memory|cpu)"
```

---

## üìä **Final Cluster Information Report Template**

**üéØ MANDATORY: Use this exact format to provide comprehensive cluster information to the user after successful deployment.**

### **Template Structure:**

```
üöÄ **ROSA CLUSTER WITH OPENSHIFT AI - DEPLOYMENT COMPLETE**

## üèóÔ∏è **CLUSTER DETAILS**
- **Cluster Name**: [cluster-name]
- **Cluster ID**: [cluster-id]
- **OpenShift Version**: [version]
- **Cluster Type**: ROSA Hosted Control Plane (HCP)
- **Region**: [aws-region] ([region-description])
- **DNS Domain**: [cluster-dns]
- **State**: ‚úÖ Ready

## üñ•Ô∏è **NODE CONFIGURATION**
- **Total Nodes**: [total-count]
- **Worker Nodes**: [worker-count] √ó [worker-instance-type] ([worker-cpu] CPU, [worker-memory] RAM each)
- **GPU Nodes**: [gpu-count] √ó [gpu-instance-type] ([gpu-cpu] CPU, [gpu-memory] RAM, [gpu-model] each)
- **Additional Pools**: [additional-pools-details]
- **Total Capacity**: [total-cpu] CPU, [total-memory] RAM, [total-gpu] GPUs

## üåê **NETWORKING**
- **API Visibility**: [Public/Private]
- **Worker Nodes**: [Public/Private]
- **VPC CIDR**: [vpc-cidr]
- **Subnets**: [subnet-details]
- **Zero Egress**: [Enabled/Disabled]

## üîê **ACCESS CREDENTIALS**

### **OpenShift Console**
- **Console URL**: [console-url]
- **Username**: [admin-username]
- **Password**: [admin-password]

### **CLI Access**
```bash
oc login [api-url] --username [admin-username] --password [admin-password] --insecure-skip-tls-verify
```

## ü§ñ **OPENSHIFT AI DASHBOARD**
- **Dashboard URL**: [dashboard-url]
- **Status**: [‚úÖ Ready/‚ö†Ô∏è Pending/‚ùå Error]
- **Authentication**: Use OpenShift cluster credentials above

## üß† **AI COMPONENTS INSTALLED**

### **Core AI Platform**
- ‚úÖ **Red Hat OpenShift AI**: [version]
- ‚úÖ **OpenShift AI Operator**: [version]
- ‚úÖ **DataScienceCluster**: [status]

### **GPU Support**
- ‚úÖ **NVIDIA GPU Operator**: [version]
- ‚úÖ **Node Feature Discovery (NFD)**: [version]
- ‚úÖ **GPU Driver Version**: [driver-version]
- ‚úÖ **CUDA Runtime**: [cuda-version]

### **Additional Components**
- ‚úÖ **OpenShift GitOps (ArgoCD)**: [version]
- ‚úÖ **OpenShift Serverless**: [version]
- ‚úÖ **OpenShift Service Mesh**: [version]
- [‚úÖ/‚ùå] **Llama Stack**: [version/status]

## üìà **CLUSTER CAPACITY & UTILIZATION**
- **CPU Utilization**: [current-usage]/[total-capacity] ([percentage]%)
- **Memory Utilization**: [current-usage]/[total-capacity] ([percentage]%)
- **GPU Utilization**: [current-usage]/[total-capacity] ([percentage]%)

## üéØ **NEXT STEPS**
1. **Access OpenShift AI Dashboard**: Use the dashboard URL above
2. **Create Data Science Projects**: Start building AI/ML workloads
3. **Deploy Models**: Use the GPU nodes for model inference
4. **Explore Jupyter Notebooks**: Begin data science workflows
5. **Monitor Resources**: Use OpenShift console for cluster monitoring

## üìö **DOCUMENTATION & SUPPORT**
- **OpenShift AI Documentation**: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed
- **ROSA Documentation**: https://docs.openshift.com/rosa/
- **GPU Workloads Guide**: https://docs.nvidia.com/datacenter/cloud-native/
- **Support**: Red Hat Customer Portal

---
**‚úÖ Deployment completed successfully on [date]**
**üîß Deployed using: [deployment-method] (Terraform/CLI)**
**üìù Configuration saved to long-term context file**
```

### **üéØ Agent Instructions:**

1. **ALWAYS** use this exact template structure
2. **REPLACE** all bracketed placeholders with actual values
3. **VERIFY** all URLs and credentials before providing
4. **SAVE** the complete information to the long-term context file
5. **TEST** dashboard accessibility before marking as Ready
6. **INCLUDE** actual version numbers for all components
7. **PROVIDE** accurate capacity and utilization metrics

### **üìù Long-Term Context File Update:**

After providing the user report, update the long-term context file with:
- Complete cluster access information
- OpenShift AI dashboard URL and status
- All component versions and status
- Node configuration and capacity details
- Deployment date and method used
